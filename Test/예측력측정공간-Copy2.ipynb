{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f339c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e58dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e965593",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret) #sample\n",
    "library(nnet) #인공신경망 , 다항 로지스틱 회귀분석\n",
    "library(e1071) # svm\n",
    "library(randomForest) # randomForest\n",
    "library(party) # 의사결정나무 ctree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e145cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#입력 데이터, 정규화할 컬럼인덱스 벡터\n",
    "fun_x <- function(f_df,cindex,targetcol){\n",
    "\n",
    "    \n",
    "# 표준화\n",
    "\n",
    "f_df2 <- f_df\n",
    "f_df2[,cindex] <- as.data.frame(scale(f_df[,cindex]))\n",
    "\n",
    "# 샘플나누기\n",
    "\n",
    "tindex = 0\n",
    "for(i in 1:ncol(f_df2)){\n",
    "    if(colnames(f_df2)[i] == targetcol){\n",
    "        tindex = i\n",
    "    }\n",
    "}\n",
    "    \n",
    "sampling <- function(){\n",
    "samp <<- createDataPartition(f_df2[,tindex] , p = 0.7,list = F)\n",
    "data.tr <<- f_df2[samp,]\n",
    "data.test <<- f_df2[-samp,]\n",
    "\n",
    "x <<- data.test[,-tindex]\n",
    "y <<- data.test[,tindex]\n",
    "    \n",
    "}    \n",
    "    \n",
    "set.seed(1235)\n",
    "\n",
    "sampling() \n",
    "    \n",
    "head(samp)\n",
    "\n",
    "#인공신경망 테스트\n",
    "ANN <- function(data.tr , data.test , x , y ){\n",
    "    \n",
    "    model.nnet <- nnet(cluster ~ . , data=data.tr, size = 3)\n",
    "    pred <- predict(model.nnet , x , type = \"class\")\n",
    "    cat(\"인공신경망의 예측력은\" ,mean(pred == y), \"입니다.\")\n",
    "#     print(pred)\n",
    "    print(table(pred,y))\n",
    "    return(mean(pred == y))\n",
    "}\n",
    "\n",
    "#의사결정 나무 \n",
    "CTREE <-function(data.tr , data.test , x , y ){\n",
    "    model.ctree <- ctree(cluster ~ . , data=data.tr)\n",
    "    pred <- predict(model.ctree , x )\n",
    "    cat(\"의사결정 나무의 예측력은\" ,mean(pred == y), \"입니다.\")\n",
    "    print(table(pred,y))\n",
    "    return(mean(pred == y))\n",
    "}\n",
    "\n",
    "#RandomForesst\n",
    "RandomForest<-function(data.tr , data.test , x , y ){\n",
    "    model.rf <- randomForest(cluster ~ . , data=data.tr, ntree = 100)\n",
    "    pred <- predict(model.rf , x )\n",
    "    cat(\"랜덤포레스트 예측력은\" ,mean(pred == y), \"입니다.\")\n",
    "    print(table(pred,y))\n",
    "    return(mean(pred == y))\n",
    "} \n",
    "\n",
    "#SVM\n",
    "SVM <-function(data.tr , data.test , x , y ){\n",
    "    model.svm <- svm(cluster ~ . , data=data.tr)\n",
    "    pred <- predict(model.svm , x )\n",
    "    cat(\"SVM의 예측력은\" ,mean(pred == y), \"입니다.\")\n",
    "    print(table(pred,y))\n",
    "    return(mean(pred == y))\n",
    "} \n",
    "\n",
    "#Logistic regression\n",
    "Logisticreg <-function(data.tr , data.test , x , y ){\n",
    "    model.log <- multinom(cluster ~ . , data=data.tr)\n",
    "    pred <- predict(model.log , x )\n",
    "    cat(\"로지스틱 회귀의 예측력은\" ,mean(pred == y), \"입니다.\")\n",
    "    print(table(pred,y))\n",
    "    return(mean(pred == y))\n",
    "} \n",
    "\n",
    " a <- rep(0,5)\n",
    "b<- rep(0,5)\n",
    "iteration <- 20\n",
    "    \n",
    "data.test2 <- f_df2[samp,]\n",
    "\n",
    "x2 <- data.test2[,-tindex]\n",
    "y2 <- data.test2[,tindex]\n",
    "for(i in 1:iteration){\n",
    "    set.seed(i)\n",
    "    a = a + c(\n",
    "            ANN(data.tr , data.test , x , y ),\n",
    "            CTREE(data.tr , data.test , x , y ),\n",
    "             RandomForest(data.tr , data.test , x , y ),\n",
    "            SVM(data.tr , data.test , x , y ),\n",
    "            Logisticreg(data.tr , data.test , x , y )\n",
    "    )\n",
    "    b = b + c(\n",
    "        ANN(data.tr , data.test2 , x2 , y2 ),\n",
    "        CTREE(data.tr , data.test2 , x2 , y2 ),\n",
    "         RandomForest(data.tr , data.test2 , x2 , y2 ),\n",
    "        SVM(data.tr , data.test2 , x2 , y2 ),\n",
    "        Logisticreg(data.tr , data.test2 , x2 , y2 )\n",
    "    )\n",
    "}\n",
    "print(a)\n",
    "print(b)\n",
    "a<- a/iteration\n",
    "b<- b/iteration\n",
    "\n",
    "result<<-data.frame(train = b,test = a)\n",
    " rownames(result) = c(\"인공신경망\",\"의사결정나무\",\"RandomForest\",\"SVM\",\"로지스틱회귀분석\")\n",
    "#rownames(result) = c(\"인공신경망\",\"의사결정나무\",\"SVM\",\"로지스틱회귀분석\")\n",
    "result\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1395eac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cluster</th><th scope=col>누적관객수</th><th scope=col>스크린점유율</th><th scope=col>배급사점수</th><th scope=col>장르_관객점수부여</th><th scope=col>감독_배우시너지</th><th scope=col>감독점수</th><th scope=col>배우점수</th><th scope=col>배우점수2</th><th scope=col>배우점수3</th><th scope=col>배우점수4</th><th scope=col>배우점수5</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1       </td><td>4039891 </td><td>23.79760</td><td>2       </td><td>10      </td><td>3       </td><td>3.000000</td><td>4.000000</td><td>4.00    </td><td>2.000000</td><td>0.00    </td><td>0.000000</td></tr>\n",
       "\t<tr><td>1       </td><td>3678156 </td><td>48.26242</td><td>2       </td><td>10      </td><td>8       </td><td>2.000000</td><td>8.250000</td><td>8.25    </td><td>4.062019</td><td>8.25    </td><td>4.062019</td></tr>\n",
       "\t<tr><td>1       </td><td>3117859 </td><td>36.90476</td><td>1       </td><td>10      </td><td>0       </td><td>2.333333</td><td>4.000000</td><td>8.25    </td><td>4.062019</td><td>8.25    </td><td>4.062019</td></tr>\n",
       "\t<tr><td>1       </td><td>4313101 </td><td>35.27944</td><td>3       </td><td>10      </td><td>3       </td><td>3.571429</td><td>6.777778</td><td>8.25    </td><td>4.509250</td><td>8.25    </td><td>4.509250</td></tr>\n",
       "\t<tr><td>1       </td><td>3024666 </td><td>21.64329</td><td>3       </td><td>10      </td><td>0       </td><td>3.500000</td><td>7.666667</td><td>8.25    </td><td>5.153456</td><td>8.25    </td><td>5.153456</td></tr>\n",
       "\t<tr><td>1       </td><td>4111237 </td><td>28.49592</td><td>2       </td><td>20      </td><td>6       </td><td>3.500000</td><td>6.500000</td><td>8.25    </td><td>5.153456</td><td>8.25    </td><td>5.153456</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " cluster & 누적관객수 & 스크린점유율 & 배급사점수 & 장르\\_관객점수부여 & 감독\\_배우시너지 & 감독점수 & 배우점수 & 배우점수2 & 배우점수3 & 배우점수4 & 배우점수5\\\\\n",
       "\\hline\n",
       "\t 1        & 4039891  & 23.79760 & 2        & 10       & 3        & 3.000000 & 4.000000 & 4.00     & 2.000000 & 0.00     & 0.000000\\\\\n",
       "\t 1        & 3678156  & 48.26242 & 2        & 10       & 8        & 2.000000 & 8.250000 & 8.25     & 4.062019 & 8.25     & 4.062019\\\\\n",
       "\t 1        & 3117859  & 36.90476 & 1        & 10       & 0        & 2.333333 & 4.000000 & 8.25     & 4.062019 & 8.25     & 4.062019\\\\\n",
       "\t 1        & 4313101  & 35.27944 & 3        & 10       & 3        & 3.571429 & 6.777778 & 8.25     & 4.509250 & 8.25     & 4.509250\\\\\n",
       "\t 1        & 3024666  & 21.64329 & 3        & 10       & 0        & 3.500000 & 7.666667 & 8.25     & 5.153456 & 8.25     & 5.153456\\\\\n",
       "\t 1        & 4111237  & 28.49592 & 2        & 20       & 6        & 3.500000 & 6.500000 & 8.25     & 5.153456 & 8.25     & 5.153456\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| cluster | 누적관객수 | 스크린점유율 | 배급사점수 | 장르_관객점수부여 | 감독_배우시너지 | 감독점수 | 배우점수 | 배우점수2 | 배우점수3 | 배우점수4 | 배우점수5 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1        | 4039891  | 23.79760 | 2        | 10       | 3        | 3.000000 | 4.000000 | 4.00     | 2.000000 | 0.00     | 0.000000 |\n",
       "| 1        | 3678156  | 48.26242 | 2        | 10       | 8        | 2.000000 | 8.250000 | 8.25     | 4.062019 | 8.25     | 4.062019 |\n",
       "| 1        | 3117859  | 36.90476 | 1        | 10       | 0        | 2.333333 | 4.000000 | 8.25     | 4.062019 | 8.25     | 4.062019 |\n",
       "| 1        | 4313101  | 35.27944 | 3        | 10       | 3        | 3.571429 | 6.777778 | 8.25     | 4.509250 | 8.25     | 4.509250 |\n",
       "| 1        | 3024666  | 21.64329 | 3        | 10       | 0        | 3.500000 | 7.666667 | 8.25     | 5.153456 | 8.25     | 5.153456 |\n",
       "| 1        | 4111237  | 28.49592 | 2        | 20       | 6        | 3.500000 | 6.500000 | 8.25     | 5.153456 | 8.25     | 5.153456 |\n",
       "\n"
      ],
      "text/plain": [
       "  cluster 누적관객수 스크린점유율 배급사점수 장르_관객점수부여 감독_배우시너지\n",
       "1 1       4039891    23.79760     2          10                3              \n",
       "2 1       3678156    48.26242     2          10                8              \n",
       "3 1       3117859    36.90476     1          10                0              \n",
       "4 1       4313101    35.27944     3          10                3              \n",
       "5 1       3024666    21.64329     3          10                0              \n",
       "6 1       4111237    28.49592     2          20                6              \n",
       "  감독점수 배우점수 배우점수2 배우점수3 배우점수4 배우점수5\n",
       "1 3.000000 4.000000 4.00      2.000000  0.00      0.000000 \n",
       "2 2.000000 8.250000 8.25      4.062019  8.25      4.062019 \n",
       "3 2.333333 4.000000 8.25      4.062019  8.25      4.062019 \n",
       "4 3.571429 6.777778 8.25      4.509250  8.25      4.509250 \n",
       "5 3.500000 7.666667 8.25      5.153456  8.25      5.153456 \n",
       "6 3.500000 6.500000 8.25      5.153456  8.25      5.153456 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df <-read.csv(\"../movies2/fortest1.csv\",fileEncoding = 'utf-8')\n",
    "df10<-df\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5398f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df<- df10[,-c(2,8,9,11,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4b69a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 <- df\n",
    "\n",
    "df2$cluster <- as.factor(df2$cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1247b915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t108 obs. of  7 variables:\n",
      " $ cluster          : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ 스크린점유율     : num  23.8 48.3 36.9 35.3 21.6 ...\n",
      " $ 배급사점수       : int  2 2 1 3 3 2 1 3 1 3 ...\n",
      " $ 장르_관객점수부여: int  10 10 10 10 10 20 10 10 10 10 ...\n",
      " $ 감독_배우시너지  : int  3 8 0 3 0 6 1 5 7 4 ...\n",
      " $ 감독점수         : num  3 2 2.33 3.57 3.5 ...\n",
      " $ 배우점수3        : num  2 4.06 4.06 4.51 5.15 ...\n"
     ]
    }
   ],
   "source": [
    "str(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2a5e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  33\n",
      "initial  value 90.795584 \n",
      "iter  10 value 43.403863\n",
      "iter  20 value 29.531476\n",
      "iter  30 value 21.905264\n",
      "iter  40 value 18.655097\n",
      "iter  50 value 17.254597\n",
      "iter  60 value 16.512235\n",
      "iter  70 value 14.179728\n",
      "iter  80 value 13.168779\n",
      "iter  90 value 12.970602\n",
      "iter 100 value 12.896790\n",
      "final  value 12.896790 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7096774 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  3  1\n",
      "   3  0  5  7\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  1\n",
      "   3  0  2  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 83.403912 \n",
      "iter  10 value 42.944178\n",
      "iter  20 value 31.695235\n",
      "iter  30 value 28.511448\n",
      "iter  40 value 27.265712\n",
      "iter  50 value 26.823637\n",
      "iter  60 value 26.655352\n",
      "iter  70 value 26.647088\n",
      "iter  80 value 26.645946\n",
      "iter  90 value 26.645921\n",
      "iter  90 value 26.645921\n",
      "iter  90 value 26.645921\n",
      "final  value 26.645921 \n",
      "converged\n",
      "인공신경망의 예측력은 0.7922078 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 28  0  0\n",
      "   2  0 13  0\n",
      "   3  3 13 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 88.655086 \n",
      "iter  10 value 46.188705\n",
      "iter  20 value 34.479187\n",
      "iter  30 value 22.190258\n",
      "iter  40 value 13.518006\n",
      "iter  50 value 10.920594\n",
      "iter  60 value 9.862496\n",
      "iter  70 value 7.584266\n",
      "iter  80 value 7.488774\n",
      "iter  90 value 7.332776\n",
      "iter 100 value 7.297323\n",
      "final  value 7.297323 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  0\n",
      "   3  0  2  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  2\n",
      "   3  0  2  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 87.754319 \n",
      "iter  10 value 47.666141\n",
      "iter  20 value 35.801327\n",
      "iter  30 value 23.634171\n",
      "iter  40 value 19.086400\n",
      "iter  50 value 17.698045\n",
      "iter  60 value 17.469193\n",
      "iter  70 value 17.443586\n",
      "iter  80 value 17.441431\n",
      "iter  90 value 17.429919\n",
      "iter 100 value 17.421167\n",
      "final  value 17.421167 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9220779 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  2  0\n",
      "   2  1 22  1\n",
      "   3  0  2 19\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 116.453412 \n",
      "iter  10 value 47.165194\n",
      "iter  20 value 29.108629\n",
      "iter  30 value 12.930780\n",
      "iter  40 value 9.236882\n",
      "iter  50 value 7.792682\n",
      "iter  60 value 6.732540\n",
      "iter  70 value 6.682511\n",
      "iter  80 value 6.618442\n",
      "iter  90 value 6.535356\n",
      "iter 100 value 6.458646\n",
      "final  value 6.458646 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9032258 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  8  0\n",
      "   3  0  1  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  8  2\n",
      "   3  0  2  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 86.374149 \n",
      "iter  10 value 44.075294\n",
      "iter  20 value 29.847979\n",
      "iter  30 value 23.321316\n",
      "iter  40 value 20.886355\n",
      "iter  50 value 20.509292\n",
      "iter  60 value 20.110199\n",
      "iter  70 value 20.013615\n",
      "iter  80 value 19.981823\n",
      "iter  90 value 19.959993\n",
      "iter 100 value 19.939271\n",
      "final  value 19.939271 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8311688 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  0  0\n",
      "   2  1 25 11\n",
      "   3  0  1  9\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 84.510174 \n",
      "iter  10 value 47.841841\n",
      "iter  20 value 34.766281\n",
      "iter  30 value 14.697078\n",
      "iter  40 value 7.179455\n",
      "iter  50 value 6.290735\n",
      "iter  60 value 6.149462\n",
      "iter  70 value 6.031270\n",
      "iter  80 value 5.971586\n",
      "iter  90 value 5.832514\n",
      "iter 100 value 5.800730\n",
      "final  value 5.800730 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7096774 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  5  0\n",
      "   2  0  6  4\n",
      "   3  0  0  4\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.9032258 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  9  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 101.597102 \n",
      "iter  10 value 44.599014\n",
      "iter  20 value 28.617940\n",
      "iter  30 value 14.371801\n",
      "iter  40 value 11.138905\n",
      "iter  50 value 10.874272\n",
      "iter  60 value 10.697828\n",
      "iter  70 value 10.607346\n",
      "iter  80 value 10.535882\n",
      "iter  90 value 10.405812\n",
      "iter 100 value 10.281855\n",
      "final  value 10.281855 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9480519 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  2  0\n",
      "   2  1 24  1\n",
      "   3  0  0 19\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 87.992719 \n",
      "iter  10 value 46.418582\n",
      "iter  20 value 24.959569\n",
      "iter  30 value 12.966232\n",
      "iter  40 value 10.681115\n",
      "iter  50 value 9.659888\n",
      "iter  60 value 8.645801\n",
      "iter  70 value 7.910682\n",
      "iter  80 value 6.860524\n",
      "iter  90 value 6.736071\n",
      "iter 100 value 6.633145\n",
      "final  value 6.633145 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  7  0\n",
      "   3  0  1  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  1\n",
      "   3  0  2  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 89.143370 \n",
      "iter  10 value 44.958897\n",
      "iter  20 value 30.839397\n",
      "iter  30 value 25.680439\n",
      "iter  40 value 21.923659\n",
      "iter  50 value 21.668229\n",
      "iter  60 value 21.573006\n",
      "iter  70 value 21.553814\n",
      "iter  80 value 21.546738\n",
      "iter  90 value 21.536340\n",
      "iter 100 value 21.501002\n",
      "final  value 21.501002 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  0 17  0\n",
      "   3  1  8 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 82.590796 \n",
      "iter  10 value 41.203465\n",
      "iter  20 value 24.393017\n",
      "iter  30 value 11.179370\n",
      "iter  40 value 8.487088\n",
      "iter  50 value 8.174995\n",
      "iter  60 value 8.150387\n",
      "final  value 8.150352 \n",
      "converged\n",
      "인공신경망의 예측력은 0.7096774 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  2  0\n",
      "   3  0  6  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  9  2\n",
      "   3  0  1  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 91.452370 \n",
      "iter  10 value 44.950103\n",
      "iter  20 value 33.084877\n",
      "iter  30 value 27.845647\n",
      "iter  40 value 26.207765\n",
      "iter  50 value 26.195179\n",
      "iter  60 value 26.156981\n",
      "iter  70 value 26.145401\n",
      "iter  80 value 26.145327\n",
      "iter  90 value 25.627911\n",
      "iter 100 value 25.596286\n",
      "final  value 25.596286 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8571429 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 28  0  0\n",
      "   2  1 18  0\n",
      "   3  2  8 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 85.019358 \n",
      "iter  10 value 43.498649\n",
      "iter  20 value 31.004598\n",
      "iter  30 value 26.097110\n",
      "iter  40 value 25.018308\n",
      "iter  50 value 24.313637\n",
      "iter  60 value 24.246921\n",
      "iter  70 value 24.156948\n",
      "iter  80 value 24.109083\n",
      "iter  90 value 24.101903\n",
      "iter 100 value 24.098891\n",
      "final  value 24.098891 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.6129032 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  0\n",
      "   2  1  2  1\n",
      "   3  1  6  7\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  8  1\n",
      "   3  0  2  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 98.650093 \n",
      "iter  10 value 45.586508\n",
      "iter  20 value 33.350610\n",
      "iter  30 value 23.950428\n",
      "iter  40 value 22.414393\n",
      "iter  50 value 21.471542\n",
      "iter  60 value 21.198748\n",
      "iter  70 value 20.906111\n",
      "iter  80 value 20.856924\n",
      "iter  90 value 20.722204\n",
      "iter 100 value 20.660878\n",
      "final  value 20.660878 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8831169 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  0  0\n",
      "   2  0 18  0\n",
      "   3  1  8 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 0.987013 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  1  0\n",
      "   2  0 25  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 93.411286 \n",
      "iter  10 value 46.101948\n",
      "iter  20 value 31.183516\n",
      "iter  30 value 18.852321\n",
      "iter  40 value 14.285798\n",
      "iter  50 value 12.877209\n",
      "iter  60 value 11.733215\n",
      "iter  70 value 11.555986\n",
      "iter  80 value 11.481729\n",
      "iter  90 value 11.274866\n",
      "iter 100 value 10.344141\n",
      "final  value 10.344141 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9032258 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 11  1  0\n",
      "   2  1  9  0\n",
      "   3  0  1  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  1\n",
      "   3  0  2  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 87.733121 \n",
      "iter  10 value 50.681729\n",
      "iter  20 value 26.927222\n",
      "iter  30 value 17.859578\n",
      "iter  40 value 15.202433\n",
      "iter  50 value 13.913753\n",
      "iter  60 value 13.442246\n",
      "iter  70 value 13.281288\n",
      "iter  80 value 13.265387\n",
      "iter  90 value 13.184135\n",
      "iter 100 value 13.160193\n",
      "final  value 13.160193 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9350649 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 23  1\n",
      "   3  0  2 19\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 112.160392 \n",
      "iter  10 value 43.893668\n",
      "iter  20 value 32.719235\n",
      "iter  30 value 29.612328\n",
      "iter  40 value 29.512990\n",
      "iter  50 value 29.467149\n",
      "iter  60 value 29.322895\n",
      "iter  70 value 29.211985\n",
      "iter  80 value 29.118071\n",
      "iter  90 value 29.074948\n",
      "iter 100 value 29.039938\n",
      "final  value 29.039938 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7096774 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 11  4  0\n",
      "   2  1  3  0\n",
      "   3  0  4  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  9  2\n",
      "   3  0  1  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 85.939088 \n",
      "iter  10 value 46.011422\n",
      "iter  20 value 29.018674\n",
      "iter  30 value 15.231723\n",
      "iter  40 value 10.723204\n",
      "iter  50 value 8.122010\n",
      "iter  60 value 6.656085\n",
      "iter  70 value 5.855858\n",
      "iter  80 value 5.769995\n",
      "iter  90 value 5.753685\n",
      "iter 100 value 5.722286\n",
      "final  value 5.722286 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.974026 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  0  0\n",
      "   2  1 25  0\n",
      "   3  0  1 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 87.845533 \n",
      "iter  10 value 39.986785\n",
      "iter  20 value 28.688953\n",
      "iter  30 value 21.390822\n",
      "iter  40 value 17.412519\n",
      "iter  50 value 17.323504\n",
      "iter  60 value 17.323248\n",
      "final  value 17.323243 \n",
      "converged\n",
      "인공신경망의 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  0\n",
      "   3  0  2  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  1\n",
      "   3  0  2  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 93.614231 \n",
      "iter  10 value 46.478123\n",
      "iter  20 value 29.112230\n",
      "iter  30 value 19.812874\n",
      "iter  40 value 12.308870\n",
      "iter  50 value 9.849483\n",
      "iter  60 value 7.365428\n",
      "iter  70 value 5.970538\n",
      "iter  80 value 5.933153\n",
      "iter  90 value 5.852246\n",
      "iter 100 value 5.801728\n",
      "final  value 5.801728 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.961039 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 24  0\n",
      "   3  0  1 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 87.962606 \n",
      "iter  10 value 48.340419\n",
      "iter  20 value 30.260027\n",
      "iter  30 value 18.582437\n",
      "iter  40 value 14.537180\n",
      "iter  50 value 9.795612\n",
      "iter  60 value 9.137451\n",
      "iter  70 value 9.017427\n",
      "iter  80 value 9.011805\n",
      "iter  90 value 9.007582\n",
      "iter 100 value 9.007145\n",
      "final  value 9.007145 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7741935 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  2\n",
      "   3  0  2  6\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.9032258 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  9  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 100.563774 \n",
      "iter  10 value 45.917026\n",
      "iter  20 value 25.401863\n",
      "iter  30 value 17.471995\n",
      "iter  40 value 15.010779\n",
      "iter  50 value 13.745544\n",
      "iter  60 value 12.838288\n",
      "iter  70 value 12.778739\n",
      "iter  80 value 12.710969\n",
      "iter  90 value 12.471847\n",
      "iter 100 value 11.821218\n",
      "final  value 11.821218 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9480519 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 22  0\n",
      "   3  0  4 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 101.137976 \n",
      "iter  10 value 47.328480\n",
      "iter  20 value 27.705271\n",
      "iter  30 value 11.469433\n",
      "iter  40 value 8.001418\n",
      "iter  50 value 6.873299\n",
      "iter  60 value 6.494610\n",
      "iter  70 value 5.849809\n",
      "iter  80 value 5.664377\n",
      "iter  90 value 5.514473\n",
      "iter 100 value 5.431202\n",
      "final  value 5.431202 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  7  0\n",
      "   3  0  1  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  2\n",
      "   3  0  2  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 86.433406 \n",
      "iter  10 value 42.090495\n",
      "iter  20 value 25.976552\n",
      "iter  30 value 16.976751\n",
      "iter  40 value 15.381169\n",
      "iter  50 value 14.129959\n",
      "iter  60 value 13.978304\n",
      "iter  70 value 13.926711\n",
      "iter  80 value 13.908053\n",
      "iter  90 value 13.881717\n",
      "iter 100 value 13.821681\n",
      "final  value 13.821681 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9480519 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 23  1\n",
      "   3  0  3 19\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 91.285076 \n",
      "iter  10 value 46.105794\n",
      "iter  20 value 28.774176\n",
      "iter  30 value 22.424477\n",
      "iter  40 value 16.716258\n",
      "iter  50 value 12.392305\n",
      "iter  60 value 11.769576\n",
      "iter  70 value 11.580518\n",
      "iter  80 value 11.499624\n",
      "iter  90 value 11.357088\n",
      "iter 100 value 11.252305\n",
      "final  value 11.252305 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 11  2  0\n",
      "   2  1  8  4\n",
      "   3  0  1  4\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  8  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 87.901117 \n",
      "iter  10 value 42.621062\n",
      "iter  20 value 31.284996\n",
      "iter  30 value 27.037304\n",
      "iter  40 value 26.731719\n",
      "iter  50 value 26.714407\n",
      "iter  60 value 26.660047\n",
      "iter  70 value 26.490216\n",
      "iter  80 value 26.436407\n",
      "iter  90 value 26.384036\n",
      "iter 100 value 25.536919\n",
      "final  value 25.536919 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8571429 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  0 16  0\n",
      "   3  1  9 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 85.094638 \n",
      "iter  10 value 47.760719\n",
      "iter  20 value 31.849803\n",
      "iter  30 value 20.412719\n",
      "iter  40 value 14.492636\n",
      "iter  50 value 11.999811\n",
      "iter  60 value 11.056383\n",
      "iter  70 value 10.949426\n",
      "iter  80 value 10.918113\n",
      "iter  90 value 10.908491\n",
      "iter 100 value 10.901933\n",
      "final  value 10.901933 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  8  2\n",
      "   3  0  1  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 88.256801 \n",
      "iter  10 value 49.600709\n",
      "iter  20 value 34.635296\n",
      "iter  30 value 23.631060\n",
      "iter  40 value 19.190940\n",
      "iter  50 value 18.403861\n",
      "iter  60 value 16.497698\n",
      "iter  70 value 14.315218\n",
      "iter  80 value 14.092597\n",
      "iter  90 value 13.920019\n",
      "iter 100 value 13.458254\n",
      "final  value 13.458254 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9480519 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 23  0\n",
      "   3  0  2 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 83.341181 \n",
      "iter  10 value 46.227331\n",
      "iter  20 value 30.459187\n",
      "iter  30 value 22.065804\n",
      "iter  40 value 16.819019\n",
      "iter  50 value 13.778833\n",
      "iter  60 value 12.547154\n",
      "iter  70 value 11.210553\n",
      "iter  80 value 8.707198\n",
      "iter  90 value 8.533253\n",
      "iter 100 value 8.291910\n",
      "final  value 8.291910 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7741935 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  7  3\n",
      "   3  0  2  5\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  7  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 91.965477 \n",
      "iter  10 value 42.001931\n",
      "iter  20 value 24.135581\n",
      "iter  30 value 15.171017\n",
      "iter  40 value 12.703123\n",
      "iter  50 value 11.162913\n",
      "iter  60 value 10.237810\n",
      "iter  70 value 9.683838\n",
      "iter  80 value 9.649489\n",
      "iter  90 value 9.610063\n",
      "iter 100 value 9.600394\n",
      "final  value 9.600394 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.961039 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 23  0\n",
      "   3  0  3 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 90.560444 \n",
      "iter  10 value 44.561538\n",
      "iter  20 value 32.869352\n",
      "iter  30 value 23.068800\n",
      "iter  40 value 17.659206\n",
      "iter  50 value 16.214111\n",
      "iter  60 value 14.553146\n",
      "iter  70 value 14.077766\n",
      "iter  80 value 13.676043\n",
      "iter  90 value 13.340849\n",
      "iter 100 value 13.132381\n",
      "final  value 13.132381 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  2  0\n",
      "   2  2  7  0\n",
      "   3  0  2  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.9032258 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  1  0\n",
      "   2  0  9  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 84.140264 \n",
      "iter  10 value 40.958917\n",
      "iter  20 value 29.377186\n",
      "iter  30 value 23.643589\n",
      "iter  40 value 20.449744\n",
      "iter  50 value 17.601373\n",
      "iter  60 value 17.036927\n",
      "iter  70 value 16.865293\n",
      "iter  80 value 16.819783\n",
      "iter  90 value 16.574260\n",
      "iter 100 value 15.871314\n",
      "final  value 15.871314 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8961039 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 29  1  0\n",
      "   2  2 20  0\n",
      "   3  0  5 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 91.623545 \n",
      "iter  10 value 45.885849\n",
      "iter  20 value 30.824751\n",
      "iter  30 value 22.697834\n",
      "iter  40 value 21.308831\n",
      "iter  50 value 19.941667\n",
      "iter  60 value 19.927934\n",
      "iter  70 value 19.920972\n",
      "iter  80 value 19.915178\n",
      "iter  90 value 19.908520\n",
      "iter 100 value 19.861111\n",
      "final  value 19.861111 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7741935 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  4  0\n",
      "   3  0  4  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  8  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 99.607476 \n",
      "iter  10 value 45.086944\n",
      "iter  20 value 28.230182\n",
      "iter  30 value 19.978168\n",
      "iter  40 value 17.728923\n",
      "iter  50 value 17.263395\n",
      "iter  60 value 14.971842\n",
      "iter  70 value 14.667900\n",
      "iter  80 value 14.640317\n",
      "iter  90 value 14.565464\n",
      "iter 100 value 14.519300\n",
      "final  value 14.519300 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9220779 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  3  0\n",
      "   2  0 23  3\n",
      "   3  0  0 17\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 84.668871 \n",
      "iter  10 value 44.745185\n",
      "iter  20 value 31.646658\n",
      "iter  30 value 22.939532\n",
      "iter  40 value 21.022295\n",
      "iter  50 value 20.978799\n",
      "iter  60 value 20.635433\n",
      "iter  70 value 20.238443\n",
      "iter  80 value 20.212420\n",
      "iter  90 value 20.198059\n",
      "iter 100 value 20.178993\n",
      "final  value 20.178993 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.7741935 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  4  0\n",
      "   3  0  4  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  7  2\n",
      "   3  0  1  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial  value 84.243940 \n",
      "iter  10 value 43.794506\n",
      "iter  20 value 30.166951\n",
      "iter  30 value 20.754076\n",
      "iter  40 value 14.883808\n",
      "iter  50 value 12.789891\n",
      "iter  60 value 11.351003\n",
      "iter  70 value 11.098178\n",
      "iter  80 value 11.012495\n",
      "iter  90 value 10.894170\n",
      "iter 100 value 10.711890\n",
      "final  value 10.711890 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9480519 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 22  0\n",
      "   3  0  4 20\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 92.401252 \n",
      "iter  10 value 47.189668\n",
      "iter  20 value 32.279117\n",
      "iter  30 value 21.546136\n",
      "iter  40 value 16.950287\n",
      "iter  50 value 14.898018\n",
      "iter  60 value 14.601440\n",
      "iter  70 value 14.486473\n",
      "iter  80 value 14.439080\n",
      "iter  90 value 14.414573\n",
      "iter 100 value 14.403560\n",
      "final  value 14.403560 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  1  8  0\n",
      "   3  1  3  8\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  8  2\n",
      "   3  0  1  6\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 99.185993 \n",
      "iter  10 value 46.762680\n",
      "iter  20 value 35.708080\n",
      "iter  30 value 29.049094\n",
      "iter  40 value 25.306706\n",
      "iter  50 value 24.589298\n",
      "iter  60 value 24.409531\n",
      "iter  70 value 24.385759\n",
      "iter  80 value 24.370078\n",
      "iter  90 value 24.098718\n",
      "iter 100 value 24.028924\n",
      "final  value 24.028924 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8831169 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 22  4\n",
      "   3  0  3 16\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "# weights:  33\n",
      "initial  value 102.693227 \n",
      "iter  10 value 45.148204\n",
      "iter  20 value 34.131468\n",
      "iter  30 value 18.875178\n",
      "iter  40 value 16.338975\n",
      "iter  50 value 14.327717\n",
      "iter  60 value 12.997445\n",
      "iter  70 value 12.370230\n",
      "iter  80 value 12.325574\n",
      "iter  90 value 12.298153\n",
      "iter 100 value 12.240156\n",
      "final  value 12.240156 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.8387097 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 11  0  0\n",
      "   2  1  8  1\n",
      "   3  0  3  7\n",
      "의사결정 나무의 예측력은 0.7419355 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  0  0\n",
      "   2  2  5  0\n",
      "   3  0  6  8\n",
      "랜덤포레스트 예측력은 0.8709677 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  2  0\n",
      "   2  0  8  1\n",
      "   3  0  1  7\n",
      "SVM의 예측력은 0.6451613 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 10  3  1\n",
      "   2  1  6  3\n",
      "   3  1  2  4\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8064516 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 12  3  0\n",
      "   2  0  6  1\n",
      "   3  0  2  7\n",
      "# weights:  33\n",
      "initial  value 93.882533 \n",
      "iter  10 value 46.469550\n",
      "iter  20 value 32.941248\n",
      "iter  30 value 16.472881\n",
      "iter  40 value 12.786269\n",
      "iter  50 value 12.099428\n",
      "iter  60 value 11.621966\n",
      "iter  70 value 11.517576\n",
      "iter  80 value 11.069194\n",
      "iter  90 value 10.544054\n",
      "iter 100 value 10.178625\n",
      "final  value 10.178625 \n",
      "stopped after 100 iterations\n",
      "인공신경망의 예측력은 0.9480519 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 23  1\n",
      "   3  0  3 19\n",
      "의사결정 나무의 예측력은 0.7272727 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 23  0  0\n",
      "   2  8 13  0\n",
      "   3  0 13 20\n",
      "랜덤포레스트 예측력은 1 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 31  0  0\n",
      "   2  0 26  0\n",
      "   3  0  0 20\n",
      "SVM의 예측력은 0.7402597 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 25  4  0\n",
      "   2  6 20  8\n",
      "   3  0  2 12\n",
      "# weights:  24 (14 variable)\n",
      "initial  value 84.593146 \n",
      "iter  10 value 47.610575\n",
      "iter  20 value 31.848905\n",
      "iter  30 value 24.271161\n",
      "iter  40 value 24.097991\n",
      "iter  50 value 24.087799\n",
      "final  value 24.075371 \n",
      "converged\n",
      "로지스틱 회귀의 예측력은 0.8701299 입니다.    y\n",
      "pred  1  2  3\n",
      "   1 30  1  0\n",
      "   2  1 20  3\n",
      "   3  0  5 17\n",
      "[1] 15.83871 14.83871 17.06452 12.90323 16.12903\n",
      "[1] 18.23377 14.54545 19.98701 14.80519 17.40260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>인공신경망</th><td>0.9116883</td><td>0.7919355</td></tr>\n",
       "\t<tr><th scope=row>의사결정나무</th><td>0.7272727</td><td>0.7419355</td></tr>\n",
       "\t<tr><th scope=row>RandomForest</th><td>0.9993506</td><td>0.8532258</td></tr>\n",
       "\t<tr><th scope=row>SVM</th><td>0.7402597</td><td>0.6451613</td></tr>\n",
       "\t<tr><th scope=row>로지스틱회귀분석</th><td>0.8701299</td><td>0.8064516</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & train & test\\\\\n",
       "\\hline\n",
       "\t인공신경망 & 0.9116883 & 0.7919355\\\\\n",
       "\t의사결정나무 & 0.7272727 & 0.7419355\\\\\n",
       "\tRandomForest & 0.9993506 & 0.8532258\\\\\n",
       "\tSVM & 0.7402597 & 0.6451613\\\\\n",
       "\t로지스틱회귀분석 & 0.8701299 & 0.8064516\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | train | test |\n",
       "|---|---|---|\n",
       "| 인공신경망 | 0.9116883 | 0.7919355 |\n",
       "| 의사결정나무 | 0.7272727 | 0.7419355 |\n",
       "| RandomForest | 0.9993506 | 0.8532258 |\n",
       "| SVM | 0.7402597 | 0.6451613 |\n",
       "| 로지스틱회귀분석 | 0.8701299 | 0.8064516 |\n",
       "\n"
      ],
      "text/plain": [
       "                 train     test     \n",
       "인공신경망       0.9116883 0.7919355\n",
       "의사결정나무     0.7272727 0.7419355\n",
       "RandomForest     0.9993506 0.8532258\n",
       "SVM              0.7402597 0.6451613\n",
       "로지스틱회귀분석 0.8701299 0.8064516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fun_x(df2,2:7,'cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8f16983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9116883</td><td>0.7919355</td></tr>\n",
       "\t<tr><td>0.7272727</td><td>0.7419355</td></tr>\n",
       "\t<tr><td>0.9993506</td><td>0.8532258</td></tr>\n",
       "\t<tr><td>0.7402597</td><td>0.6451613</td></tr>\n",
       "\t<tr><td>0.8701299</td><td>0.8064516</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.9116883 & 0.7919355\\\\\n",
       "\t 0.7272727 & 0.7419355\\\\\n",
       "\t 0.9993506 & 0.8532258\\\\\n",
       "\t 0.7402597 & 0.6451613\\\\\n",
       "\t 0.8701299 & 0.8064516\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.9116883 | 0.7919355 |\n",
       "| 0.7272727 | 0.7419355 |\n",
       "| 0.9993506 | 0.8532258 |\n",
       "| 0.7402597 | 0.6451613 |\n",
       "| 0.8701299 | 0.8064516 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.9116883 0.7919355\n",
       "2 0.7272727 0.7419355\n",
       "3 0.9993506 0.8532258\n",
       "4 0.7402597 0.6451613\n",
       "5 0.8701299 0.8064516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#배우점수4\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1088f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9116883</td><td>0.7919355</td></tr>\n",
       "\t<tr><td>0.7272727</td><td>0.7419355</td></tr>\n",
       "\t<tr><td>0.9993506</td><td>0.8532258</td></tr>\n",
       "\t<tr><td>0.7402597</td><td>0.6451613</td></tr>\n",
       "\t<tr><td>0.8701299</td><td>0.8064516</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.9116883 & 0.7919355\\\\\n",
       "\t 0.7272727 & 0.7419355\\\\\n",
       "\t 0.9993506 & 0.8532258\\\\\n",
       "\t 0.7402597 & 0.6451613\\\\\n",
       "\t 0.8701299 & 0.8064516\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.9116883 | 0.7919355 |\n",
       "| 0.7272727 | 0.7419355 |\n",
       "| 0.9993506 | 0.8532258 |\n",
       "| 0.7402597 | 0.6451613 |\n",
       "| 0.8701299 | 0.8064516 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.9116883 0.7919355\n",
       "2 0.7272727 0.7419355\n",
       "3 0.9993506 0.8532258\n",
       "4 0.7402597 0.6451613\n",
       "5 0.8701299 0.8064516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#배우점수3\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25970830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9915584</td><td>0.9306452</td></tr>\n",
       "\t<tr><td>0.9220779</td><td>0.9354839</td></tr>\n",
       "\t<tr><td>1.0000000</td><td>0.9500000</td></tr>\n",
       "\t<tr><td>0.8831169</td><td>0.8064516</td></tr>\n",
       "\t<tr><td>0.9740260</td><td>0.9354839</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.9915584 & 0.9306452\\\\\n",
       "\t 0.9220779 & 0.9354839\\\\\n",
       "\t 1.0000000 & 0.9500000\\\\\n",
       "\t 0.8831169 & 0.8064516\\\\\n",
       "\t 0.9740260 & 0.9354839\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.9915584 | 0.9306452 |\n",
       "| 0.9220779 | 0.9354839 |\n",
       "| 1.0000000 | 0.9500000 |\n",
       "| 0.8831169 | 0.8064516 |\n",
       "| 0.9740260 | 0.9354839 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.9915584 0.9306452\n",
       "2 0.9220779 0.9354839\n",
       "3 1.0000000 0.9500000\n",
       "4 0.8831169 0.8064516\n",
       "5 0.9740260 0.9354839"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#배우점수2\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6c4b72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.8337662</td><td>0.6516129</td></tr>\n",
       "\t<tr><td>0.6363636</td><td>0.6129032</td></tr>\n",
       "\t<tr><td>1.0000000</td><td>0.6709677</td></tr>\n",
       "\t<tr><td>0.7662338</td><td>0.6774194</td></tr>\n",
       "\t<tr><td>0.7272727</td><td>0.7096774</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.8337662 & 0.6516129\\\\\n",
       "\t 0.6363636 & 0.6129032\\\\\n",
       "\t 1.0000000 & 0.6709677\\\\\n",
       "\t 0.7662338 & 0.6774194\\\\\n",
       "\t 0.7272727 & 0.7096774\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.8337662 | 0.6516129 |\n",
       "| 0.6363636 | 0.6129032 |\n",
       "| 1.0000000 | 0.6709677 |\n",
       "| 0.7662338 | 0.6774194 |\n",
       "| 0.7272727 | 0.7096774 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.8337662 0.6516129\n",
       "2 0.6363636 0.6129032\n",
       "3 1.0000000 0.6709677\n",
       "4 0.7662338 0.6774194\n",
       "5 0.7272727 0.7096774"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SVM, 의사결정나무, 로지스틱회귀분석은 할때 마다 난수 생성을 안하는 듯 (샘플링 난수에만 영향을 받는다.)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea70701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9311688</td><td>0.8032258</td></tr>\n",
       "\t<tr><td>0.8441558</td><td>0.8387097</td></tr>\n",
       "\t<tr><td>0.9993506</td><td>0.8612903</td></tr>\n",
       "\t<tr><td>0.8701299</td><td>0.8709677</td></tr>\n",
       "\t<tr><td>0.8441558</td><td>0.8387097</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.9311688 & 0.8032258\\\\\n",
       "\t 0.8441558 & 0.8387097\\\\\n",
       "\t 0.9993506 & 0.8612903\\\\\n",
       "\t 0.8701299 & 0.8709677\\\\\n",
       "\t 0.8441558 & 0.8387097\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.9311688 | 0.8032258 |\n",
       "| 0.8441558 | 0.8387097 |\n",
       "| 0.9993506 | 0.8612903 |\n",
       "| 0.8701299 | 0.8709677 |\n",
       "| 0.8441558 | 0.8387097 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.9311688 0.8032258\n",
       "2 0.8441558 0.8387097\n",
       "3 0.9993506 0.8612903\n",
       "4 0.8701299 0.8709677\n",
       "5 0.8441558 0.8387097"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1234\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f6a43075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9311688</td><td>0.8032258</td></tr>\n",
       "\t<tr><td>0.8441558</td><td>0.8387097</td></tr>\n",
       "\t<tr><td>0.9993506</td><td>0.8612903</td></tr>\n",
       "\t<tr><td>0.8701299</td><td>0.8709677</td></tr>\n",
       "\t<tr><td>0.8441558</td><td>0.8387097</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.9311688 & 0.8032258\\\\\n",
       "\t 0.8441558 & 0.8387097\\\\\n",
       "\t 0.9993506 & 0.8612903\\\\\n",
       "\t 0.8701299 & 0.8709677\\\\\n",
       "\t 0.8441558 & 0.8387097\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.9311688 | 0.8032258 |\n",
       "| 0.8441558 | 0.8387097 |\n",
       "| 0.9993506 | 0.8612903 |\n",
       "| 0.8701299 | 0.8709677 |\n",
       "| 0.8441558 | 0.8387097 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.9311688 0.8032258\n",
       "2 0.8441558 0.8387097\n",
       "3 0.9993506 0.8612903\n",
       "4 0.8701299 0.8709677\n",
       "5 0.8441558 0.8387097"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1235\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "99814497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>train</th><th scope=col>test</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9577922</td><td>0.6322581</td></tr>\n",
       "\t<tr><td>0.8701299</td><td>0.7741935</td></tr>\n",
       "\t<tr><td>0.9993506</td><td>0.7854839</td></tr>\n",
       "\t<tr><td>0.9090909</td><td>0.7419355</td></tr>\n",
       "\t<tr><td>0.8701299</td><td>0.7096774</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " train & test\\\\\n",
       "\\hline\n",
       "\t 0.9577922 & 0.6322581\\\\\n",
       "\t 0.8701299 & 0.7741935\\\\\n",
       "\t 0.9993506 & 0.7854839\\\\\n",
       "\t 0.9090909 & 0.7419355\\\\\n",
       "\t 0.8701299 & 0.7096774\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| train | test |\n",
       "|---|---|\n",
       "| 0.9577922 | 0.6322581 |\n",
       "| 0.8701299 | 0.7741935 |\n",
       "| 0.9993506 | 0.7854839 |\n",
       "| 0.9090909 | 0.7419355 |\n",
       "| 0.8701299 | 0.7096774 |\n",
       "\n"
      ],
      "text/plain": [
       "  train     test     \n",
       "1 0.9577922 0.6322581\n",
       "2 0.8701299 0.7741935\n",
       "3 0.9993506 0.7854839\n",
       "4 0.9090909 0.7419355\n",
       "5 0.8701299 0.7096774"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1236\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc406e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요까지가 머신러닝\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c76f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015f3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d89bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ac89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99966d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de442150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8004c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "genre4 <- df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c24d2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale<-function(x){\n",
    "    return((x-min(x))/(max(x)-min(x)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cec6a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5<-as.data.frame(lapply(df[,-1],scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1761cb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>감독_주연조연시너지수</th><th scope=col>스크린점유율</th><th scope=col>배우점수</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.4285714</td><td>0.5726291</td><td>0.625    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " 감독\\_주연조연시너지수 & 스크린점유율 & 배우점수\\\\\n",
       "\\hline\n",
       "\t 0.4285714 & 0.5726291 & 0.625    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 감독_주연조연시너지수 | 스크린점유율 | 배우점수 |\n",
       "|---|---|---|\n",
       "| 0.4285714 | 0.5726291 | 0.625     |\n",
       "\n"
      ],
      "text/plain": [
       "  감독_주연조연시너지수 스크린점유율 배우점수\n",
       "1 0.4285714             0.5726291    0.625   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c9c505e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cluster</th><th scope=col>감독_주연조연시너지수</th><th scope=col>스크린점유율</th><th scope=col>배우점수</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 7232387</td><td>6       </td><td>46.96312</td><td>16.0    </td></tr>\n",
       "\t<tr><td> 2535450</td><td>0       </td><td>17.21519</td><td> 7.0    </td></tr>\n",
       "\t<tr><td> 2963652</td><td>0       </td><td>29.36088</td><td> 6.5    </td></tr>\n",
       "\t<tr><td> 2242510</td><td>3       </td><td>41.13475</td><td> 7.0    </td></tr>\n",
       "\t<tr><td> 4039891</td><td>3       </td><td>23.79760</td><td> 4.0    </td></tr>\n",
       "\t<tr><td>12811213</td><td>0       </td><td>39.65201</td><td>25.0    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " cluster & 감독\\_주연조연시너지수 & 스크린점유율 & 배우점수\\\\\n",
       "\\hline\n",
       "\t  7232387 & 6        & 46.96312 & 16.0    \\\\\n",
       "\t  2535450 & 0        & 17.21519 &  7.0    \\\\\n",
       "\t  2963652 & 0        & 29.36088 &  6.5    \\\\\n",
       "\t  2242510 & 3        & 41.13475 &  7.0    \\\\\n",
       "\t  4039891 & 3        & 23.79760 &  4.0    \\\\\n",
       "\t 12811213 & 0        & 39.65201 & 25.0    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| cluster | 감독_주연조연시너지수 | 스크린점유율 | 배우점수 |\n",
       "|---|---|---|---|\n",
       "|  7232387 | 6        | 46.96312 | 16.0     |\n",
       "|  2535450 | 0        | 17.21519 |  7.0     |\n",
       "|  2963652 | 0        | 29.36088 |  6.5     |\n",
       "|  2242510 | 3        | 41.13475 |  7.0     |\n",
       "|  4039891 | 3        | 23.79760 |  4.0     |\n",
       "| 12811213 | 0        | 39.65201 | 25.0     |\n",
       "\n"
      ],
      "text/plain": [
       "  cluster  감독_주연조연시너지수 스크린점유율 배우점수\n",
       "1  7232387 6                     46.96312     16.0    \n",
       "2  2535450 0                     17.21519      7.0    \n",
       "3  2963652 0                     29.36088      6.5    \n",
       "4  2242510 3                     41.13475      7.0    \n",
       "5  4039891 3                     23.79760      4.0    \n",
       "6 12811213 0                     39.65201     25.0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89187f",
   "metadata": {},
   "source": [
    "- lapply 함수는 list + apply를 의미하는 이름의 함수 \n",
    "- 실행 결과가 list 형태로 출력되는데, 리스트(list)의 인자는 length( 데이터 ) 만큼 생성된다.\n",
    "- 참고로 데이터프레임(data frame)인 경우 length( 데이터 )의 결과는 변수의 개수(열의 개수)이고, 리스트인 경우 length( 데이터 )는 리스트 인자의 개수이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "31aad7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-nearest neighbor regression model"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = knnreg(cluster ~ ., data = df)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e37d40fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in createDataPartition(df, p = 0.7):\n",
      "“Some classes have no records (  ) and these will be ignored”Warning message in createDataPartition(df, p = 0.7):\n",
      "“Some classes have a single record (  ) and these will be selected for the sample”"
     ]
    }
   ],
   "source": [
    "\n",
    "inTrain <- createDataPartition(df, p = .7)[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f047ec08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>cluster</th><th scope=col>감독_주연조연시너지수</th><th scope=col>스크린점유율</th><th scope=col>배우점수</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 7232387</td><td>6       </td><td>46.96312</td><td>16.0    </td></tr>\n",
       "\t<tr><td> 2535450</td><td>0       </td><td>17.21519</td><td> 7.0    </td></tr>\n",
       "\t<tr><td> 2963652</td><td>0       </td><td>29.36088</td><td> 6.5    </td></tr>\n",
       "\t<tr><td> 2242510</td><td>3       </td><td>41.13475</td><td> 7.0    </td></tr>\n",
       "\t<tr><td> 4039891</td><td>3       </td><td>23.79760</td><td> 4.0    </td></tr>\n",
       "\t<tr><td>12811213</td><td>0       </td><td>39.65201</td><td>25.0    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " cluster & 감독\\_주연조연시너지수 & 스크린점유율 & 배우점수\\\\\n",
       "\\hline\n",
       "\t  7232387 & 6        & 46.96312 & 16.0    \\\\\n",
       "\t  2535450 & 0        & 17.21519 &  7.0    \\\\\n",
       "\t  2963652 & 0        & 29.36088 &  6.5    \\\\\n",
       "\t  2242510 & 3        & 41.13475 &  7.0    \\\\\n",
       "\t  4039891 & 3        & 23.79760 &  4.0    \\\\\n",
       "\t 12811213 & 0        & 39.65201 & 25.0    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| cluster | 감독_주연조연시너지수 | 스크린점유율 | 배우점수 |\n",
       "|---|---|---|---|\n",
       "|  7232387 | 6        | 46.96312 | 16.0     |\n",
       "|  2535450 | 0        | 17.21519 |  7.0     |\n",
       "|  2963652 | 0        | 29.36088 |  6.5     |\n",
       "|  2242510 | 3        | 41.13475 |  7.0     |\n",
       "|  4039891 | 3        | 23.79760 |  4.0     |\n",
       "| 12811213 | 0        | 39.65201 | 25.0     |\n",
       "\n"
      ],
      "text/plain": [
       "  cluster  감독_주연조연시너지수 스크린점유율 배우점수\n",
       "1  7232387 6                     46.96312     16.0    \n",
       "2  2535450 0                     17.21519      7.0    \n",
       "3  2963652 0                     29.36088      6.5    \n",
       "4  2242510 3                     41.13475      7.0    \n",
       "5  4039891 3                     23.79760      4.0    \n",
       "6 12811213 0                     39.65201     25.0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f6388f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t209 obs. of  4 variables:\n",
      " $ cluster              : int  7232387 2535450 2963652 2242510 4039891 12811213 3678156 2598859 2368267 4529876 ...\n",
      " $ 감독_주연조연시너지수: int  6 0 0 3 3 0 8 4 0 4 ...\n",
      " $ 스크린점유율         : num  47 17.2 29.4 41.1 23.8 ...\n",
      " $ 배우점수             : num  16 7 6.5 7 4 25 8.25 5 3 5 ...\n"
     ]
    }
   ],
   "source": [
    "str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5a2a9373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAYAAAD958/bAAAEDmlDQ1BrQ0dDb2xvclNwYWNl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\nI5QOAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOT RUN {\n",
    "\n",
    "samp <- createDataPartition(df[,1] , p = 0.7,list = F)\n",
    "data.tr <- df[samp,]\n",
    "data.test <- df[-samp,]\n",
    "\n",
    "x <- data.test[,-1]\n",
    "y <- data.test[,1]\n",
    "\n",
    "fit <- knnreg(data.tr[,2:4],data.tr[,1], k = 3)\n",
    "\n",
    "plot(y, predict(fit, x))\n",
    "\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8d7d3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>8885933.33333333</li>\n",
       "\t<li>9550709</li>\n",
       "\t<li>5634348</li>\n",
       "\t<li>2619323</li>\n",
       "\t<li>4770253.66666667</li>\n",
       "\t<li>7164133.33333333</li>\n",
       "\t<li>4616623.33333333</li>\n",
       "\t<li>7842121</li>\n",
       "\t<li>3577097.66666667</li>\n",
       "\t<li>4904591.33333333</li>\n",
       "\t<li>2701066.66666667</li>\n",
       "\t<li>11660470.6666667</li>\n",
       "\t<li>6238847</li>\n",
       "\t<li>2128651.33333333</li>\n",
       "\t<li>5577357</li>\n",
       "\t<li>4583601.66666667</li>\n",
       "\t<li>2551018</li>\n",
       "\t<li>9991332.33333333</li>\n",
       "\t<li>2819592.33333333</li>\n",
       "\t<li>3108072</li>\n",
       "\t<li>9550709</li>\n",
       "\t<li>2383605.33333333</li>\n",
       "\t<li>2109987.33333333</li>\n",
       "\t<li>2759870</li>\n",
       "\t<li>4146821.66666667</li>\n",
       "\t<li>6290677.66666667</li>\n",
       "\t<li>5577357</li>\n",
       "\t<li>4894923.33333333</li>\n",
       "\t<li>2028071</li>\n",
       "\t<li>2952525.33333333</li>\n",
       "\t<li>9116441</li>\n",
       "\t<li>3748545.66666667</li>\n",
       "\t<li>2919036</li>\n",
       "\t<li>2845782.66666667</li>\n",
       "\t<li>2834251.66666667</li>\n",
       "\t<li>2839539</li>\n",
       "\t<li>2028071</li>\n",
       "\t<li>9550709</li>\n",
       "\t<li>2717816.33333333</li>\n",
       "\t<li>5634348</li>\n",
       "\t<li>4887445.66666667</li>\n",
       "\t<li>3323447.66666667</li>\n",
       "\t<li>4045531.66666667</li>\n",
       "\t<li>4532423.66666667</li>\n",
       "\t<li>4403405</li>\n",
       "\t<li>4626263.33333333</li>\n",
       "\t<li>2571617</li>\n",
       "\t<li>9980781.33333333</li>\n",
       "\t<li>5246402</li>\n",
       "\t<li>4904591.33333333</li>\n",
       "\t<li>9200756</li>\n",
       "\t<li>4944669.66666667</li>\n",
       "\t<li>2587678.33333333</li>\n",
       "\t<li>2308924</li>\n",
       "\t<li>5001495.66666667</li>\n",
       "\t<li>4312958.33333333</li>\n",
       "\t<li>3771988.33333333</li>\n",
       "\t<li>5092684.33333333</li>\n",
       "\t<li>9032815.33333333</li>\n",
       "\t<li>4926661</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 8885933.33333333\n",
       "\\item 9550709\n",
       "\\item 5634348\n",
       "\\item 2619323\n",
       "\\item 4770253.66666667\n",
       "\\item 7164133.33333333\n",
       "\\item 4616623.33333333\n",
       "\\item 7842121\n",
       "\\item 3577097.66666667\n",
       "\\item 4904591.33333333\n",
       "\\item 2701066.66666667\n",
       "\\item 11660470.6666667\n",
       "\\item 6238847\n",
       "\\item 2128651.33333333\n",
       "\\item 5577357\n",
       "\\item 4583601.66666667\n",
       "\\item 2551018\n",
       "\\item 9991332.33333333\n",
       "\\item 2819592.33333333\n",
       "\\item 3108072\n",
       "\\item 9550709\n",
       "\\item 2383605.33333333\n",
       "\\item 2109987.33333333\n",
       "\\item 2759870\n",
       "\\item 4146821.66666667\n",
       "\\item 6290677.66666667\n",
       "\\item 5577357\n",
       "\\item 4894923.33333333\n",
       "\\item 2028071\n",
       "\\item 2952525.33333333\n",
       "\\item 9116441\n",
       "\\item 3748545.66666667\n",
       "\\item 2919036\n",
       "\\item 2845782.66666667\n",
       "\\item 2834251.66666667\n",
       "\\item 2839539\n",
       "\\item 2028071\n",
       "\\item 9550709\n",
       "\\item 2717816.33333333\n",
       "\\item 5634348\n",
       "\\item 4887445.66666667\n",
       "\\item 3323447.66666667\n",
       "\\item 4045531.66666667\n",
       "\\item 4532423.66666667\n",
       "\\item 4403405\n",
       "\\item 4626263.33333333\n",
       "\\item 2571617\n",
       "\\item 9980781.33333333\n",
       "\\item 5246402\n",
       "\\item 4904591.33333333\n",
       "\\item 9200756\n",
       "\\item 4944669.66666667\n",
       "\\item 2587678.33333333\n",
       "\\item 2308924\n",
       "\\item 5001495.66666667\n",
       "\\item 4312958.33333333\n",
       "\\item 3771988.33333333\n",
       "\\item 5092684.33333333\n",
       "\\item 9032815.33333333\n",
       "\\item 4926661\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 8885933.33333333\n",
       "2. 9550709\n",
       "3. 5634348\n",
       "4. 2619323\n",
       "5. 4770253.66666667\n",
       "6. 7164133.33333333\n",
       "7. 4616623.33333333\n",
       "8. 7842121\n",
       "9. 3577097.66666667\n",
       "10. 4904591.33333333\n",
       "11. 2701066.66666667\n",
       "12. 11660470.6666667\n",
       "13. 6238847\n",
       "14. 2128651.33333333\n",
       "15. 5577357\n",
       "16. 4583601.66666667\n",
       "17. 2551018\n",
       "18. 9991332.33333333\n",
       "19. 2819592.33333333\n",
       "20. 3108072\n",
       "21. 9550709\n",
       "22. 2383605.33333333\n",
       "23. 2109987.33333333\n",
       "24. 2759870\n",
       "25. 4146821.66666667\n",
       "26. 6290677.66666667\n",
       "27. 5577357\n",
       "28. 4894923.33333333\n",
       "29. 2028071\n",
       "30. 2952525.33333333\n",
       "31. 9116441\n",
       "32. 3748545.66666667\n",
       "33. 2919036\n",
       "34. 2845782.66666667\n",
       "35. 2834251.66666667\n",
       "36. 2839539\n",
       "37. 2028071\n",
       "38. 9550709\n",
       "39. 2717816.33333333\n",
       "40. 5634348\n",
       "41. 4887445.66666667\n",
       "42. 3323447.66666667\n",
       "43. 4045531.66666667\n",
       "44. 4532423.66666667\n",
       "45. 4403405\n",
       "46. 4626263.33333333\n",
       "47. 2571617\n",
       "48. 9980781.33333333\n",
       "49. 5246402\n",
       "50. 4904591.33333333\n",
       "51. 9200756\n",
       "52. 4944669.66666667\n",
       "53. 2587678.33333333\n",
       "54. 2308924\n",
       "55. 5001495.66666667\n",
       "56. 4312958.33333333\n",
       "57. 3771988.33333333\n",
       "58. 5092684.33333333\n",
       "59. 9032815.33333333\n",
       "60. 4926661\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  8885933  9550709  5634348  2619323  4770254  7164133  4616623  7842121\n",
       " [9]  3577098  4904591  2701067 11660471  6238847  2128651  5577357  4583602\n",
       "[17]  2551018  9991332  2819592  3108072  9550709  2383605  2109987  2759870\n",
       "[25]  4146822  6290678  5577357  4894923  2028071  2952525  9116441  3748546\n",
       "[33]  2919036  2845783  2834252  2839539  2028071  9550709  2717816  5634348\n",
       "[41]  4887446  3323448  4045532  4532424  4403405  4626263  2571617  9980781\n",
       "[49]  5246402  4904591  9200756  4944670  2587678  2308924  5001496  4312958\n",
       "[57]  3771988  5092684  9032815  4926661"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(fit, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5550ebf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "   Cell Contents\n",
      "|-------------------------|\n",
      "|                       N |\n",
      "|           N / Row Total |\n",
      "|           N / Col Total |\n",
      "|         N / Table Total |\n",
      "|-------------------------|\n",
      "\n",
      " \n",
      "Total Observations in Table:  59 \n",
      "\n",
      " \n",
      "                  | genre5_model \n",
      "genre5.testLabels |   5317959 |   5508017 |   8666208 |   9707581 |  12706483 |  12983330 |  14411775 |  17615039 | Row Total | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                0 |         1 |         6 |         4 |         0 |         0 |         6 |         6 |         4 |        27 | \n",
      "                  |     0.037 |     0.222 |     0.148 |     0.000 |     0.000 |     0.222 |     0.222 |     0.148 |     0.458 | \n",
      "                  |     0.500 |     0.545 |     0.444 |     0.000 |     0.000 |     0.353 |     0.500 |     0.667 |           | \n",
      "                  |     0.017 |     0.102 |     0.068 |     0.000 |     0.000 |     0.102 |     0.102 |     0.068 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                1 |         0 |         0 |         0 |         0 |         0 |         1 |         0 |         0 |         1 | \n",
      "                  |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     1.000 |     0.000 |     0.000 |     0.017 | \n",
      "                  |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.059 |     0.000 |     0.000 |           | \n",
      "                  |     0.000 |     0.000 |     0.000 |     0.000 |     0.000 |     0.017 |     0.000 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                2 |         0 |         0 |         1 |         0 |         0 |         1 |         1 |         0 |         3 | \n",
      "                  |     0.000 |     0.000 |     0.333 |     0.000 |     0.000 |     0.333 |     0.333 |     0.000 |     0.051 | \n",
      "                  |     0.000 |     0.000 |     0.111 |     0.000 |     0.000 |     0.059 |     0.083 |     0.000 |           | \n",
      "                  |     0.000 |     0.000 |     0.017 |     0.000 |     0.000 |     0.017 |     0.017 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                3 |         0 |         2 |         0 |         0 |         0 |         2 |         0 |         0 |         4 | \n",
      "                  |     0.000 |     0.500 |     0.000 |     0.000 |     0.000 |     0.500 |     0.000 |     0.000 |     0.068 | \n",
      "                  |     0.000 |     0.182 |     0.000 |     0.000 |     0.000 |     0.118 |     0.000 |     0.000 |           | \n",
      "                  |     0.000 |     0.034 |     0.000 |     0.000 |     0.000 |     0.034 |     0.000 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                4 |         0 |         1 |         0 |         0 |         0 |         2 |         0 |         0 |         3 | \n",
      "                  |     0.000 |     0.333 |     0.000 |     0.000 |     0.000 |     0.667 |     0.000 |     0.000 |     0.051 | \n",
      "                  |     0.000 |     0.091 |     0.000 |     0.000 |     0.000 |     0.118 |     0.000 |     0.000 |           | \n",
      "                  |     0.000 |     0.017 |     0.000 |     0.000 |     0.000 |     0.034 |     0.000 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                5 |         0 |         1 |         0 |         0 |         0 |         1 |         1 |         0 |         3 | \n",
      "                  |     0.000 |     0.333 |     0.000 |     0.000 |     0.000 |     0.333 |     0.333 |     0.000 |     0.051 | \n",
      "                  |     0.000 |     0.091 |     0.000 |     0.000 |     0.000 |     0.059 |     0.083 |     0.000 |           | \n",
      "                  |     0.000 |     0.017 |     0.000 |     0.000 |     0.000 |     0.017 |     0.017 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                6 |         0 |         1 |         1 |         1 |         1 |         2 |         2 |         0 |         8 | \n",
      "                  |     0.000 |     0.125 |     0.125 |     0.125 |     0.125 |     0.250 |     0.250 |     0.000 |     0.136 | \n",
      "                  |     0.000 |     0.091 |     0.111 |     1.000 |     1.000 |     0.118 |     0.167 |     0.000 |           | \n",
      "                  |     0.000 |     0.017 |     0.017 |     0.017 |     0.017 |     0.034 |     0.034 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                7 |         1 |         0 |         1 |         0 |         0 |         0 |         1 |         1 |         4 | \n",
      "                  |     0.250 |     0.000 |     0.250 |     0.000 |     0.000 |     0.000 |     0.250 |     0.250 |     0.068 | \n",
      "                  |     0.500 |     0.000 |     0.111 |     0.000 |     0.000 |     0.000 |     0.083 |     0.167 |           | \n",
      "                  |     0.017 |     0.000 |     0.017 |     0.000 |     0.000 |     0.000 |     0.017 |     0.017 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                8 |         0 |         0 |         1 |         0 |         0 |         1 |         0 |         0 |         2 | \n",
      "                  |     0.000 |     0.000 |     0.500 |     0.000 |     0.000 |     0.500 |     0.000 |     0.000 |     0.034 | \n",
      "                  |     0.000 |     0.000 |     0.111 |     0.000 |     0.000 |     0.059 |     0.000 |     0.000 |           | \n",
      "                  |     0.000 |     0.000 |     0.017 |     0.000 |     0.000 |     0.017 |     0.000 |     0.000 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "                9 |         0 |         0 |         1 |         0 |         0 |         1 |         1 |         1 |         4 | \n",
      "                  |     0.000 |     0.000 |     0.250 |     0.000 |     0.000 |     0.250 |     0.250 |     0.250 |     0.068 | \n",
      "                  |     0.000 |     0.000 |     0.111 |     0.000 |     0.000 |     0.059 |     0.083 |     0.167 |           | \n",
      "                  |     0.000 |     0.000 |     0.017 |     0.000 |     0.000 |     0.017 |     0.017 |     0.017 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "     Column Total |         2 |        11 |         9 |         1 |         1 |        17 |        12 |         6 |        59 | \n",
      "                  |     0.034 |     0.186 |     0.153 |     0.017 |     0.017 |     0.288 |     0.203 |     0.102 |           | \n",
      "------------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "CrossTable(x = genre5.testLabels, y = genre5_model, prop.chisq = FALSE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5d048928",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(knitr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2a7ff6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f5fa7aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>10403</li>\n",
       "\t<li>624</li>\n",
       "\t<li>-1564105589</li>\n",
       "\t<li>103469648</li>\n",
       "\t<li>-280462831</li>\n",
       "\t<li>-984769378</li>\n",
       "\t<li>-1934069625</li>\n",
       "\t<li>2112878364</li>\n",
       "\t<li>-3060371</li>\n",
       "\t<li>-923367094</li>\n",
       "\t<li>-72437181</li>\n",
       "\t<li>473245352</li>\n",
       "\t<li>1882094729</li>\n",
       "\t<li>-1374310730</li>\n",
       "\t<li>804398527</li>\n",
       "\t<li>-695079692</li>\n",
       "\t<li>685187941</li>\n",
       "\t<li>-998737694</li>\n",
       "\t<li>-344045829</li>\n",
       "\t<li>1152685568</li>\n",
       "\t<li>-969269759</li>\n",
       "\t<li>-837741618</li>\n",
       "\t<li>-176401929</li>\n",
       "\t<li>917384652</li>\n",
       "\t<li>-811988899</li>\n",
       "\t<li>421686138</li>\n",
       "\t<li>1266631347</li>\n",
       "\t<li>771653720</li>\n",
       "\t<li>1101610617</li>\n",
       "\t<li>1798056934</li>\n",
       "\t<li>1015010607</li>\n",
       "\t<li>-983557724</li>\n",
       "\t<li>149281877</li>\n",
       "\t<li>-1466515182</li>\n",
       "\t<li>1771603307</li>\n",
       "\t<li>-749451856</li>\n",
       "\t<li>-944390671</li>\n",
       "\t<li>-450930946</li>\n",
       "\t<li>1753321319</li>\n",
       "\t<li>-747696004</li>\n",
       "\t<li>71466829</li>\n",
       "\t<li>1224989098</li>\n",
       "\t<li>-2083721437</li>\n",
       "\t<li>-496810488</li>\n",
       "\t<li>-1709867927</li>\n",
       "\t<li>-152111850</li>\n",
       "\t<li>-723361633</li>\n",
       "\t<li>1489924692</li>\n",
       "\t<li>192139589</li>\n",
       "\t<li>-559671998</li>\n",
       "\t<li>-1279565861</li>\n",
       "\t<li>-792403616</li>\n",
       "\t<li>242899425</li>\n",
       "\t<li>678127150</li>\n",
       "\t<li>945760471</li>\n",
       "\t<li>572366636</li>\n",
       "\t<li>1912189501</li>\n",
       "\t<li>-1522674726</li>\n",
       "\t<li>1243527059</li>\n",
       "\t<li>-1585547336</li>\n",
       "\t<li>907163225</li>\n",
       "\t<li>1873873478</li>\n",
       "\t<li>2022754319</li>\n",
       "\t<li>-1373112572</li>\n",
       "\t<li>1955594805</li>\n",
       "\t<li>-1448905358</li>\n",
       "\t<li>-1706174901</li>\n",
       "\t<li>1518430480</li>\n",
       "\t<li>1963389393</li>\n",
       "\t<li>44581214</li>\n",
       "\t<li>-311681465</li>\n",
       "\t<li>-1151018532</li>\n",
       "\t<li>145662253</li>\n",
       "\t<li>1932745226</li>\n",
       "\t<li>901487619</li>\n",
       "\t<li>707466600</li>\n",
       "\t<li>167668809</li>\n",
       "\t<li>1485138806</li>\n",
       "\t<li>348261247</li>\n",
       "\t<li>-2055755852</li>\n",
       "\t<li>-1677103323</li>\n",
       "\t<li>-581443166</li>\n",
       "\t<li>-1753814853</li>\n",
       "\t<li>1019534528</li>\n",
       "\t<li>-2053470783</li>\n",
       "\t<li>1531504782</li>\n",
       "\t<li>-1245745225</li>\n",
       "\t<li>-1297104756</li>\n",
       "\t<li>-1005564899</li>\n",
       "\t<li>554134586</li>\n",
       "\t<li>1068145779</li>\n",
       "\t<li>1107566360</li>\n",
       "\t<li>838409785</li>\n",
       "\t<li>-918611802</li>\n",
       "\t<li>1953311471</li>\n",
       "\t<li>-242711452</li>\n",
       "\t<li>-579921899</li>\n",
       "\t<li>239360466</li>\n",
       "\t<li>1058903851</li>\n",
       "\t<li>-1567998864</li>\n",
       "\t<li>1781798321</li>\n",
       "\t<li>-964666434</li>\n",
       "\t<li>-718267097</li>\n",
       "\t<li>1177113404</li>\n",
       "\t<li>-1685212403</li>\n",
       "\t<li>1973226090</li>\n",
       "\t<li>850573539</li>\n",
       "\t<li>1701090504</li>\n",
       "\t<li>-505328599</li>\n",
       "\t<li>-1636757034</li>\n",
       "\t<li>-1337383329</li>\n",
       "\t<li>132484372</li>\n",
       "\t<li>-2012218107</li>\n",
       "\t<li>-1045701118</li>\n",
       "\t<li>-1360469605</li>\n",
       "\t<li>-980645856</li>\n",
       "\t<li>-594370143</li>\n",
       "\t<li>-1253991698</li>\n",
       "\t<li>357901975</li>\n",
       "\t<li>-1900244500</li>\n",
       "\t<li>1918227965</li>\n",
       "\t<li>-1063832422</li>\n",
       "\t<li>458944851</li>\n",
       "\t<li>2003269240</li>\n",
       "\t<li>1431696921</li>\n",
       "\t<li>-1452386554</li>\n",
       "\t<li>-1630732849</li>\n",
       "\t<li>-1864777276</li>\n",
       "\t<li>-822403595</li>\n",
       "\t<li>-1651413454</li>\n",
       "\t<li>-29374453</li>\n",
       "\t<li>-1639530544</li>\n",
       "\t<li>372582801</li>\n",
       "\t<li>-1522555362</li>\n",
       "\t<li>897944583</li>\n",
       "\t<li>806648988</li>\n",
       "\t<li>123188461</li>\n",
       "\t<li>173599434</li>\n",
       "\t<li>-1209383485</li>\n",
       "\t<li>1911014440</li>\n",
       "\t<li>-1078584311</li>\n",
       "\t<li>-532027338</li>\n",
       "\t<li>1143976255</li>\n",
       "\t<li>-1217387916</li>\n",
       "\t<li>-1191216411</li>\n",
       "\t<li>-1732769182</li>\n",
       "\t<li>-1370928517</li>\n",
       "\t<li>-1812733056</li>\n",
       "\t<li>-1067799167</li>\n",
       "\t<li>1357741390</li>\n",
       "\t<li>1524125047</li>\n",
       "\t<li>144446284</li>\n",
       "\t<li>-448639011</li>\n",
       "\t<li>1141189882</li>\n",
       "\t<li>-395856333</li>\n",
       "\t<li>360742360</li>\n",
       "\t<li>1008778745</li>\n",
       "\t<li>-1915304602</li>\n",
       "\t<li>1114128559</li>\n",
       "\t<li>-1183600860</li>\n",
       "\t<li>279712725</li>\n",
       "\t<li>715305618</li>\n",
       "\t<li>434923755</li>\n",
       "\t<li>747565872</li>\n",
       "\t<li>-469619343</li>\n",
       "\t<li>-545382274</li>\n",
       "\t<li>-2145096985</li>\n",
       "\t<li>-511814148</li>\n",
       "\t<li>1384425165</li>\n",
       "\t<li>2004810538</li>\n",
       "\t<li>513426083</li>\n",
       "\t<li>-1718836344</li>\n",
       "\t<li>-1116414999</li>\n",
       "\t<li>-2119700842</li>\n",
       "\t<li>1227729951</li>\n",
       "\t<li>-1754306604</li>\n",
       "\t<li>1414523077</li>\n",
       "\t<li>2073323202</li>\n",
       "\t<li>-439344293</li>\n",
       "\t<li>-1127026976</li>\n",
       "\t<li>-638932639</li>\n",
       "\t<li>350523310</li>\n",
       "\t<li>-436149161</li>\n",
       "\t<li>514213036</li>\n",
       "\t<li>1095612861</li>\n",
       "\t<li>-144091814</li>\n",
       "\t<li>-838276333</li>\n",
       "\t<li>1546073400</li>\n",
       "\t<li>-28215847</li>\n",
       "\t<li>1074815942</li>\n",
       "\t<li>-2047413361</li>\n",
       "\t<li>-995210108</li>\n",
       "\t<li>-1510344267</li>\n",
       "\t<li>-1802492174</li>\n",
       "\t<li>1885043147</li>\n",
       "\t<li>406509200</li>\n",
       "\t<li>982720849</li>\n",
       "\t<li>-2116826402</li>\n",
       "\t<li>-2101036601</li>\n",
       "\t<li>1858002780</li>\n",
       "\t<li>1066174637</li>\n",
       "\t<li>-1893254262</li>\n",
       "\t<li>-604328061</li>\n",
       "\t<li>-1842662680</li>\n",
       "\t<li>1897237449</li>\n",
       "\t<li>841164022</li>\n",
       "\t<li>335222527</li>\n",
       "\t<li>-683975372</li>\n",
       "\t<li>-1149679963</li>\n",
       "\t<li>-1889995998</li>\n",
       "\t<li>1102408763</li>\n",
       "\t<li>1090628160</li>\n",
       "\t<li>-835021503</li>\n",
       "\t<li>-1279340018</li>\n",
       "\t<li>1921444663</li>\n",
       "\t<li>2066949644</li>\n",
       "\t<li>1727009693</li>\n",
       "\t<li>-1294225990</li>\n",
       "\t<li>259428339</li>\n",
       "\t<li>-147612520</li>\n",
       "\t<li>803216825</li>\n",
       "\t<li>-709676506</li>\n",
       "\t<li>1815156335</li>\n",
       "\t<li>937531876</li>\n",
       "\t<li>-832778347</li>\n",
       "\t<li>-965620910</li>\n",
       "\t<li>2076506795</li>\n",
       "\t<li>404908528</li>\n",
       "\t<li>2095056177</li>\n",
       "\t<li>1691919678</li>\n",
       "\t<li>1730050215</li>\n",
       "\t<li>-1741809476</li>\n",
       "\t<li>1290230413</li>\n",
       "\t<li>-1352029206</li>\n",
       "\t<li>-2126279581</li>\n",
       "\t<li>2107339336</li>\n",
       "\t<li>-326095959</li>\n",
       "\t<li>-313291946</li>\n",
       "\t<li>-716181025</li>\n",
       "\t<li>-768867692</li>\n",
       "\t<li>-1946971003</li>\n",
       "\t<li>85831554</li>\n",
       "\t<li>1244734747</li>\n",
       "\t<li>223876512</li>\n",
       "\t<li>1044541729</li>\n",
       "\t<li>-1407957906</li>\n",
       "\t<li>404906519</li>\n",
       "\t<li>1956296556</li>\n",
       "\t<li>-224305795</li>\n",
       "\t<li>-629918182</li>\n",
       "\t<li>199795923</li>\n",
       "\t<li>-25316360</li>\n",
       "\t<li>-523979367</li>\n",
       "\t<li>-1336463226</li>\n",
       "\t<li>-741430961</li>\n",
       "\t<li>-999975100</li>\n",
       "\t<li>88905077</li>\n",
       "\t<li>-1218469966</li>\n",
       "\t<li>1382083467</li>\n",
       "\t<li>-820138672</li>\n",
       "\t<li>165730577</li>\n",
       "\t<li>757378974</li>\n",
       "\t<li>-1293310073</li>\n",
       "\t<li>-903609828</li>\n",
       "\t<li>-1257431955</li>\n",
       "\t<li>-1034007478</li>\n",
       "\t<li>-1146300093</li>\n",
       "\t<li>-373988952</li>\n",
       "\t<li>-1109607543</li>\n",
       "\t<li>-86957642</li>\n",
       "\t<li>-1713095489</li>\n",
       "\t<li>261707764</li>\n",
       "\t<li>-1623797147</li>\n",
       "\t<li>435854306</li>\n",
       "\t<li>595283451</li>\n",
       "\t<li>-89247488</li>\n",
       "\t<li>-956678911</li>\n",
       "\t<li>1216145102</li>\n",
       "\t<li>1250642167</li>\n",
       "\t<li>221575372</li>\n",
       "\t<li>1020893021</li>\n",
       "\t<li>1581969018</li>\n",
       "\t<li>1050094003</li>\n",
       "\t<li>-170034344</li>\n",
       "\t<li>-1661518471</li>\n",
       "\t<li>2106875622</li>\n",
       "\t<li>2005380143</li>\n",
       "\t<li>1200768164</li>\n",
       "\t<li>37833557</li>\n",
       "\t<li>1785832466</li>\n",
       "\t<li>-1503179669</li>\n",
       "\t<li>-872111952</li>\n",
       "\t<li>1015913713</li>\n",
       "\t<li>1263528446</li>\n",
       "\t<li>1205749351</li>\n",
       "\t<li>486054780</li>\n",
       "\t<li>1853214285</li>\n",
       "\t<li>1042095274</li>\n",
       "\t<li>1416533539</li>\n",
       "\t<li>-799997688</li>\n",
       "\t<li>-286049431</li>\n",
       "\t<li>-298588138</li>\n",
       "\t<li>1248851871</li>\n",
       "\t<li>1121672532</li>\n",
       "\t<li>180027461</li>\n",
       "\t<li>386381890</li>\n",
       "\t<li>-1916016933</li>\n",
       "\t<li>-841221024</li>\n",
       "\t<li>22673633</li>\n",
       "\t<li>-1617905362</li>\n",
       "\t<li>-646340649</li>\n",
       "\t<li>-212211156</li>\n",
       "\t<li>1511047485</li>\n",
       "\t<li>-1166551334</li>\n",
       "\t<li>1052384915</li>\n",
       "\t<li>-852823368</li>\n",
       "\t<li>1819260249</li>\n",
       "\t<li>922926406</li>\n",
       "\t<li>-300671217</li>\n",
       "\t<li>-893410812</li>\n",
       "\t<li>-1196232395</li>\n",
       "\t<li>-289417102</li>\n",
       "\t<li>-972022453</li>\n",
       "\t<li>-1985002480</li>\n",
       "\t<li>1809731793</li>\n",
       "\t<li>-68004770</li>\n",
       "\t<li>1672762695</li>\n",
       "\t<li>1426318556</li>\n",
       "\t<li>731476013</li>\n",
       "\t<li>616439050</li>\n",
       "\t<li>817939203</li>\n",
       "\t<li>-1756999576</li>\n",
       "\t<li>97233737</li>\n",
       "\t<li>-1491870090</li>\n",
       "\t<li>-1414847873</li>\n",
       "\t<li>1263145652</li>\n",
       "\t<li>536354341</li>\n",
       "\t<li>1365050530</li>\n",
       "\t<li>-447025221</li>\n",
       "\t<li>934901696</li>\n",
       "\t<li>-2108054335</li>\n",
       "\t<li>-1813529714</li>\n",
       "\t<li>-257595721</li>\n",
       "\t<li>-2124313716</li>\n",
       "\t<li>448715549</li>\n",
       "\t<li>-149754054</li>\n",
       "\t<li>-1081506957</li>\n",
       "\t<li>-532801000</li>\n",
       "\t<li>-752476871</li>\n",
       "\t<li>574245798</li>\n",
       "\t<li>-1439956497</li>\n",
       "\t<li>-2092585116</li>\n",
       "\t<li>1478067989</li>\n",
       "\t<li>1600273618</li>\n",
       "\t<li>-1684840917</li>\n",
       "\t<li>1861588848</li>\n",
       "\t<li>-355797839</li>\n",
       "\t<li>1201925822</li>\n",
       "\t<li>-1608264665</li>\n",
       "\t<li>-492970436</li>\n",
       "\t<li>1525678605</li>\n",
       "\t<li>72961386</li>\n",
       "\t<li>1373331427</li>\n",
       "\t<li>275599304</li>\n",
       "\t<li>73272105</li>\n",
       "\t<li>1359545558</li>\n",
       "\t<li>1582153055</li>\n",
       "\t<li>876443668</li>\n",
       "\t<li>1818635269</li>\n",
       "\t<li>705855746</li>\n",
       "\t<li>576743579</li>\n",
       "\t<li>-719412448</li>\n",
       "\t<li>-621723487</li>\n",
       "\t<li>-736498194</li>\n",
       "\t<li>398892439</li>\n",
       "\t<li>-1113334548</li>\n",
       "\t<li>190571773</li>\n",
       "\t<li>-1472972902</li>\n",
       "\t<li>-1875027885</li>\n",
       "\t<li>-152112776</li>\n",
       "\t<li>-787319527</li>\n",
       "\t<li>-791475706</li>\n",
       "\t<li>-91794225</li>\n",
       "\t<li>-763597628</li>\n",
       "\t<li>1273826549</li>\n",
       "\t<li>-479145678</li>\n",
       "\t<li>-1389818101</li>\n",
       "\t<li>-827352368</li>\n",
       "\t<li>139167889</li>\n",
       "\t<li>50116894</li>\n",
       "\t<li>-219888889</li>\n",
       "\t<li>-501315684</li>\n",
       "\t<li>653362157</li>\n",
       "\t<li>-150557238</li>\n",
       "\t<li>-722047805</li>\n",
       "\t<li>2040397608</li>\n",
       "\t<li>1755470601</li>\n",
       "\t<li>1672174390</li>\n",
       "\t<li>-552613825</li>\n",
       "\t<li>890080628</li>\n",
       "\t<li>-1182979611</li>\n",
       "\t<li>239086946</li>\n",
       "\t<li>-652979845</li>\n",
       "\t<li>786660992</li>\n",
       "\t<li>-1743205247</li>\n",
       "\t<li>-624996274</li>\n",
       "\t<li>848643191</li>\n",
       "\t<li>1517870668</li>\n",
       "\t<li>1952440029</li>\n",
       "\t<li>-302796806</li>\n",
       "\t<li>-1676829389</li>\n",
       "\t<li>1159035096</li>\n",
       "\t<li>-500384519</li>\n",
       "\t<li>543488102</li>\n",
       "\t<li>165549999</li>\n",
       "\t<li>1169938980</li>\n",
       "\t<li>1000702677</li>\n",
       "\t<li>-1375496814</li>\n",
       "\t<li>487141355</li>\n",
       "\t<li>-407548368</li>\n",
       "\t<li>257428593</li>\n",
       "\t<li>-829115522</li>\n",
       "\t<li>-1381031449</li>\n",
       "\t<li>467525884</li>\n",
       "\t<li>1981150669</li>\n",
       "\t<li>-1562493398</li>\n",
       "\t<li>-213259869</li>\n",
       "\t<li>2091933320</li>\n",
       "\t<li>747674345</li>\n",
       "\t<li>-1567432298</li>\n",
       "\t<li>-2035727585</li>\n",
       "\t<li>-1324199212</li>\n",
       "\t<li>213194693</li>\n",
       "\t<li>1996360130</li>\n",
       "\t<li>967748187</li>\n",
       "\t<li>-1176499744</li>\n",
       "\t<li>1120421985</li>\n",
       "\t<li>-294657362</li>\n",
       "\t<li>-2134287529</li>\n",
       "\t<li>-1237807188</li>\n",
       "\t<li>1514326205</li>\n",
       "\t<li>1953060954</li>\n",
       "\t<li>-365800941</li>\n",
       "\t<li>1787408440</li>\n",
       "\t<li>-26413863</li>\n",
       "\t<li>981997254</li>\n",
       "\t<li>-555201905</li>\n",
       "\t<li>-1772357756</li>\n",
       "\t<li>180021429</li>\n",
       "\t<li>-30242318</li>\n",
       "\t<li>-1452556085</li>\n",
       "\t<li>-455167600</li>\n",
       "\t<li>1189642321</li>\n",
       "\t<li>386129374</li>\n",
       "\t<li>2117791943</li>\n",
       "\t<li>70511196</li>\n",
       "\t<li>-355117139</li>\n",
       "\t<li>972553866</li>\n",
       "\t<li>34461315</li>\n",
       "\t<li>796683752</li>\n",
       "\t<li>-970929463</li>\n",
       "\t<li>492279798</li>\n",
       "\t<li>-1982714369</li>\n",
       "\t<li>933480500</li>\n",
       "\t<li>-1484393051</li>\n",
       "\t<li>-379316702</li>\n",
       "\t<li>275215163</li>\n",
       "\t<li>-689158848</li>\n",
       "\t<li>1610069057</li>\n",
       "\t<li>566469902</li>\n",
       "\t<li>-1642405321</li>\n",
       "\t<li>-616894196</li>\n",
       "\t<li>2105320093</li>\n",
       "\t<li>1940730042</li>\n",
       "\t<li>-1646037261</li>\n",
       "\t<li>1931712408</li>\n",
       "\t<li>-1714742087</li>\n",
       "\t<li>-1798019802</li>\n",
       "\t<li>1549659503</li>\n",
       "\t<li>-1447770908</li>\n",
       "\t<li>-660259179</li>\n",
       "\t<li>521514578</li>\n",
       "\t<li>-1400323669</li>\n",
       "\t<li>-586955536</li>\n",
       "\t<li>-235609039</li>\n",
       "\t<li>350369854</li>\n",
       "\t<li>1849700263</li>\n",
       "\t<li>-1149721668</li>\n",
       "\t<li>-475551347</li>\n",
       "\t<li>2053893866</li>\n",
       "\t<li>1920611171</li>\n",
       "\t<li>333065544</li>\n",
       "\t<li>659221161</li>\n",
       "\t<li>798064214</li>\n",
       "\t<li>-113080097</li>\n",
       "\t<li>-2078675564</li>\n",
       "\t<li>124240773</li>\n",
       "\t<li>-158707070</li>\n",
       "\t<li>-982078437</li>\n",
       "\t<li>-757059424</li>\n",
       "\t<li>1889472545</li>\n",
       "\t<li>1397921646</li>\n",
       "\t<li>-2109613801</li>\n",
       "\t<li>2144862828</li>\n",
       "\t<li>1518693501</li>\n",
       "\t<li>-1344849638</li>\n",
       "\t<li>-161936429</li>\n",
       "\t<li>-692375816</li>\n",
       "\t<li>-1539361639</li>\n",
       "\t<li>-253631610</li>\n",
       "\t<li>1089929295</li>\n",
       "\t<li>-1860287932</li>\n",
       "\t<li>14451829</li>\n",
       "\t<li>1740964530</li>\n",
       "\t<li>479736459</li>\n",
       "\t<li>-755201968</li>\n",
       "\t<li>1333082129</li>\n",
       "\t<li>-859323746</li>\n",
       "\t<li>-478749049</li>\n",
       "\t<li>235146524</li>\n",
       "\t<li>2063919981</li>\n",
       "\t<li>-1370353846</li>\n",
       "\t<li>-775487421</li>\n",
       "\t<li>396467368</li>\n",
       "\t<li>-1106838903</li>\n",
       "\t<li>-2133289802</li>\n",
       "\t<li>-1045277761</li>\n",
       "\t<li>2110571252</li>\n",
       "\t<li>-439189147</li>\n",
       "\t<li>998817506</li>\n",
       "\t<li>1561613563</li>\n",
       "\t<li>-426521600</li>\n",
       "\t<li>-239707135</li>\n",
       "\t<li>766818766</li>\n",
       "\t<li>-2131345417</li>\n",
       "\t<li>107463628</li>\n",
       "\t<li>701834845</li>\n",
       "\t<li>2030006650</li>\n",
       "\t<li>1321930931</li>\n",
       "\t<li>2032694872</li>\n",
       "\t<li>-1983824775</li>\n",
       "\t<li>1548259814</li>\n",
       "\t<li>661357359</li>\n",
       "\t<li>-1980731484</li>\n",
       "\t<li>450411093</li>\n",
       "\t<li>995657490</li>\n",
       "\t<li>-1949166741</li>\n",
       "\t<li>-1247741008</li>\n",
       "\t<li>-1704887311</li>\n",
       "\t<li>256670974</li>\n",
       "\t<li>-1617494681</li>\n",
       "\t<li>1949181564</li>\n",
       "\t<li>-2023416499</li>\n",
       "\t<li>-1413324886</li>\n",
       "\t<li>-919847645</li>\n",
       "\t<li>-1800750072</li>\n",
       "\t<li>1951201897</li>\n",
       "\t<li>80010006</li>\n",
       "\t<li>-1411805537</li>\n",
       "\t<li>940853332</li>\n",
       "\t<li>943599429</li>\n",
       "\t<li>1635212098</li>\n",
       "\t<li>2004381147</li>\n",
       "\t<li>920590176</li>\n",
       "\t<li>1547016161</li>\n",
       "\t<li>662834222</li>\n",
       "\t<li>1240471255</li>\n",
       "\t<li>2101490988</li>\n",
       "\t<li>-538718147</li>\n",
       "\t<li>-1422009894</li>\n",
       "\t<li>510756243</li>\n",
       "\t<li>-1438421576</li>\n",
       "\t<li>843658329</li>\n",
       "\t<li>815820870</li>\n",
       "\t<li>-2039253489</li>\n",
       "\t<li>-41726716</li>\n",
       "\t<li>-99491787</li>\n",
       "\t<li>149437298</li>\n",
       "\t<li>678323275</li>\n",
       "\t<li>1607016208</li>\n",
       "\t<li>162639825</li>\n",
       "\t<li>2030593886</li>\n",
       "\t<li>-1067938745</li>\n",
       "\t<li>307163100</li>\n",
       "\t<li>-1690288339</li>\n",
       "\t<li>-724246518</li>\n",
       "\t<li>501344771</li>\n",
       "\t<li>1355647848</li>\n",
       "\t<li>-1340806583</li>\n",
       "\t<li>-85044874</li>\n",
       "\t<li>1550858623</li>\n",
       "\t<li>-230130252</li>\n",
       "\t<li>807587109</li>\n",
       "\t<li>493758370</li>\n",
       "\t<li>1356527291</li>\n",
       "\t<li>-728100160</li>\n",
       "\t<li>622117825</li>\n",
       "\t<li>-2091741554</li>\n",
       "\t<li>-387490377</li>\n",
       "\t<li>-1631627636</li>\n",
       "\t<li>757688861</li>\n",
       "\t<li>-1364561350</li>\n",
       "\t<li>-125539725</li>\n",
       "\t<li>635704600</li>\n",
       "\t<li>30350393</li>\n",
       "\t<li>327253670</li>\n",
       "\t<li>-1329145617</li>\n",
       "\t<li>-2127635868</li>\n",
       "\t<li>-1375734251</li>\n",
       "\t<li>1267474386</li>\n",
       "\t<li>-1130027733</li>\n",
       "\t<li>-1739787664</li>\n",
       "\t<li>-799157327</li>\n",
       "\t<li>1922269630</li>\n",
       "\t<li>-1082946777</li>\n",
       "\t<li>-1195480772</li>\n",
       "\t<li>84824333</li>\n",
       "\t<li>396464234</li>\n",
       "\t<li>-1323301149</li>\n",
       "\t<li>2111965896</li>\n",
       "\t<li>1398196777</li>\n",
       "\t<li>-286459946</li>\n",
       "\t<li>1412322399</li>\n",
       "\t<li>398549780</li>\n",
       "\t<li>989354757</li>\n",
       "\t<li>814031874</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 10403\n",
       "\\item 624\n",
       "\\item -1564105589\n",
       "\\item 103469648\n",
       "\\item -280462831\n",
       "\\item -984769378\n",
       "\\item -1934069625\n",
       "\\item 2112878364\n",
       "\\item -3060371\n",
       "\\item -923367094\n",
       "\\item -72437181\n",
       "\\item 473245352\n",
       "\\item 1882094729\n",
       "\\item -1374310730\n",
       "\\item 804398527\n",
       "\\item -695079692\n",
       "\\item 685187941\n",
       "\\item -998737694\n",
       "\\item -344045829\n",
       "\\item 1152685568\n",
       "\\item -969269759\n",
       "\\item -837741618\n",
       "\\item -176401929\n",
       "\\item 917384652\n",
       "\\item -811988899\n",
       "\\item 421686138\n",
       "\\item 1266631347\n",
       "\\item 771653720\n",
       "\\item 1101610617\n",
       "\\item 1798056934\n",
       "\\item 1015010607\n",
       "\\item -983557724\n",
       "\\item 149281877\n",
       "\\item -1466515182\n",
       "\\item 1771603307\n",
       "\\item -749451856\n",
       "\\item -944390671\n",
       "\\item -450930946\n",
       "\\item 1753321319\n",
       "\\item -747696004\n",
       "\\item 71466829\n",
       "\\item 1224989098\n",
       "\\item -2083721437\n",
       "\\item -496810488\n",
       "\\item -1709867927\n",
       "\\item -152111850\n",
       "\\item -723361633\n",
       "\\item 1489924692\n",
       "\\item 192139589\n",
       "\\item -559671998\n",
       "\\item -1279565861\n",
       "\\item -792403616\n",
       "\\item 242899425\n",
       "\\item 678127150\n",
       "\\item 945760471\n",
       "\\item 572366636\n",
       "\\item 1912189501\n",
       "\\item -1522674726\n",
       "\\item 1243527059\n",
       "\\item -1585547336\n",
       "\\item 907163225\n",
       "\\item 1873873478\n",
       "\\item 2022754319\n",
       "\\item -1373112572\n",
       "\\item 1955594805\n",
       "\\item -1448905358\n",
       "\\item -1706174901\n",
       "\\item 1518430480\n",
       "\\item 1963389393\n",
       "\\item 44581214\n",
       "\\item -311681465\n",
       "\\item -1151018532\n",
       "\\item 145662253\n",
       "\\item 1932745226\n",
       "\\item 901487619\n",
       "\\item 707466600\n",
       "\\item 167668809\n",
       "\\item 1485138806\n",
       "\\item 348261247\n",
       "\\item -2055755852\n",
       "\\item -1677103323\n",
       "\\item -581443166\n",
       "\\item -1753814853\n",
       "\\item 1019534528\n",
       "\\item -2053470783\n",
       "\\item 1531504782\n",
       "\\item -1245745225\n",
       "\\item -1297104756\n",
       "\\item -1005564899\n",
       "\\item 554134586\n",
       "\\item 1068145779\n",
       "\\item 1107566360\n",
       "\\item 838409785\n",
       "\\item -918611802\n",
       "\\item 1953311471\n",
       "\\item -242711452\n",
       "\\item -579921899\n",
       "\\item 239360466\n",
       "\\item 1058903851\n",
       "\\item -1567998864\n",
       "\\item 1781798321\n",
       "\\item -964666434\n",
       "\\item -718267097\n",
       "\\item 1177113404\n",
       "\\item -1685212403\n",
       "\\item 1973226090\n",
       "\\item 850573539\n",
       "\\item 1701090504\n",
       "\\item -505328599\n",
       "\\item -1636757034\n",
       "\\item -1337383329\n",
       "\\item 132484372\n",
       "\\item -2012218107\n",
       "\\item -1045701118\n",
       "\\item -1360469605\n",
       "\\item -980645856\n",
       "\\item -594370143\n",
       "\\item -1253991698\n",
       "\\item 357901975\n",
       "\\item -1900244500\n",
       "\\item 1918227965\n",
       "\\item -1063832422\n",
       "\\item 458944851\n",
       "\\item 2003269240\n",
       "\\item 1431696921\n",
       "\\item -1452386554\n",
       "\\item -1630732849\n",
       "\\item -1864777276\n",
       "\\item -822403595\n",
       "\\item -1651413454\n",
       "\\item -29374453\n",
       "\\item -1639530544\n",
       "\\item 372582801\n",
       "\\item -1522555362\n",
       "\\item 897944583\n",
       "\\item 806648988\n",
       "\\item 123188461\n",
       "\\item 173599434\n",
       "\\item -1209383485\n",
       "\\item 1911014440\n",
       "\\item -1078584311\n",
       "\\item -532027338\n",
       "\\item 1143976255\n",
       "\\item -1217387916\n",
       "\\item -1191216411\n",
       "\\item -1732769182\n",
       "\\item -1370928517\n",
       "\\item -1812733056\n",
       "\\item -1067799167\n",
       "\\item 1357741390\n",
       "\\item 1524125047\n",
       "\\item 144446284\n",
       "\\item -448639011\n",
       "\\item 1141189882\n",
       "\\item -395856333\n",
       "\\item 360742360\n",
       "\\item 1008778745\n",
       "\\item -1915304602\n",
       "\\item 1114128559\n",
       "\\item -1183600860\n",
       "\\item 279712725\n",
       "\\item 715305618\n",
       "\\item 434923755\n",
       "\\item 747565872\n",
       "\\item -469619343\n",
       "\\item -545382274\n",
       "\\item -2145096985\n",
       "\\item -511814148\n",
       "\\item 1384425165\n",
       "\\item 2004810538\n",
       "\\item 513426083\n",
       "\\item -1718836344\n",
       "\\item -1116414999\n",
       "\\item -2119700842\n",
       "\\item 1227729951\n",
       "\\item -1754306604\n",
       "\\item 1414523077\n",
       "\\item 2073323202\n",
       "\\item -439344293\n",
       "\\item -1127026976\n",
       "\\item -638932639\n",
       "\\item 350523310\n",
       "\\item -436149161\n",
       "\\item 514213036\n",
       "\\item 1095612861\n",
       "\\item -144091814\n",
       "\\item -838276333\n",
       "\\item 1546073400\n",
       "\\item -28215847\n",
       "\\item 1074815942\n",
       "\\item -2047413361\n",
       "\\item -995210108\n",
       "\\item -1510344267\n",
       "\\item -1802492174\n",
       "\\item 1885043147\n",
       "\\item 406509200\n",
       "\\item 982720849\n",
       "\\item -2116826402\n",
       "\\item -2101036601\n",
       "\\item 1858002780\n",
       "\\item 1066174637\n",
       "\\item -1893254262\n",
       "\\item -604328061\n",
       "\\item -1842662680\n",
       "\\item 1897237449\n",
       "\\item 841164022\n",
       "\\item 335222527\n",
       "\\item -683975372\n",
       "\\item -1149679963\n",
       "\\item -1889995998\n",
       "\\item 1102408763\n",
       "\\item 1090628160\n",
       "\\item -835021503\n",
       "\\item -1279340018\n",
       "\\item 1921444663\n",
       "\\item 2066949644\n",
       "\\item 1727009693\n",
       "\\item -1294225990\n",
       "\\item 259428339\n",
       "\\item -147612520\n",
       "\\item 803216825\n",
       "\\item -709676506\n",
       "\\item 1815156335\n",
       "\\item 937531876\n",
       "\\item -832778347\n",
       "\\item -965620910\n",
       "\\item 2076506795\n",
       "\\item 404908528\n",
       "\\item 2095056177\n",
       "\\item 1691919678\n",
       "\\item 1730050215\n",
       "\\item -1741809476\n",
       "\\item 1290230413\n",
       "\\item -1352029206\n",
       "\\item -2126279581\n",
       "\\item 2107339336\n",
       "\\item -326095959\n",
       "\\item -313291946\n",
       "\\item -716181025\n",
       "\\item -768867692\n",
       "\\item -1946971003\n",
       "\\item 85831554\n",
       "\\item 1244734747\n",
       "\\item 223876512\n",
       "\\item 1044541729\n",
       "\\item -1407957906\n",
       "\\item 404906519\n",
       "\\item 1956296556\n",
       "\\item -224305795\n",
       "\\item -629918182\n",
       "\\item 199795923\n",
       "\\item -25316360\n",
       "\\item -523979367\n",
       "\\item -1336463226\n",
       "\\item -741430961\n",
       "\\item -999975100\n",
       "\\item 88905077\n",
       "\\item -1218469966\n",
       "\\item 1382083467\n",
       "\\item -820138672\n",
       "\\item 165730577\n",
       "\\item 757378974\n",
       "\\item -1293310073\n",
       "\\item -903609828\n",
       "\\item -1257431955\n",
       "\\item -1034007478\n",
       "\\item -1146300093\n",
       "\\item -373988952\n",
       "\\item -1109607543\n",
       "\\item -86957642\n",
       "\\item -1713095489\n",
       "\\item 261707764\n",
       "\\item -1623797147\n",
       "\\item 435854306\n",
       "\\item 595283451\n",
       "\\item -89247488\n",
       "\\item -956678911\n",
       "\\item 1216145102\n",
       "\\item 1250642167\n",
       "\\item 221575372\n",
       "\\item 1020893021\n",
       "\\item 1581969018\n",
       "\\item 1050094003\n",
       "\\item -170034344\n",
       "\\item -1661518471\n",
       "\\item 2106875622\n",
       "\\item 2005380143\n",
       "\\item 1200768164\n",
       "\\item 37833557\n",
       "\\item 1785832466\n",
       "\\item -1503179669\n",
       "\\item -872111952\n",
       "\\item 1015913713\n",
       "\\item 1263528446\n",
       "\\item 1205749351\n",
       "\\item 486054780\n",
       "\\item 1853214285\n",
       "\\item 1042095274\n",
       "\\item 1416533539\n",
       "\\item -799997688\n",
       "\\item -286049431\n",
       "\\item -298588138\n",
       "\\item 1248851871\n",
       "\\item 1121672532\n",
       "\\item 180027461\n",
       "\\item 386381890\n",
       "\\item -1916016933\n",
       "\\item -841221024\n",
       "\\item 22673633\n",
       "\\item -1617905362\n",
       "\\item -646340649\n",
       "\\item -212211156\n",
       "\\item 1511047485\n",
       "\\item -1166551334\n",
       "\\item 1052384915\n",
       "\\item -852823368\n",
       "\\item 1819260249\n",
       "\\item 922926406\n",
       "\\item -300671217\n",
       "\\item -893410812\n",
       "\\item -1196232395\n",
       "\\item -289417102\n",
       "\\item -972022453\n",
       "\\item -1985002480\n",
       "\\item 1809731793\n",
       "\\item -68004770\n",
       "\\item 1672762695\n",
       "\\item 1426318556\n",
       "\\item 731476013\n",
       "\\item 616439050\n",
       "\\item 817939203\n",
       "\\item -1756999576\n",
       "\\item 97233737\n",
       "\\item -1491870090\n",
       "\\item -1414847873\n",
       "\\item 1263145652\n",
       "\\item 536354341\n",
       "\\item 1365050530\n",
       "\\item -447025221\n",
       "\\item 934901696\n",
       "\\item -2108054335\n",
       "\\item -1813529714\n",
       "\\item -257595721\n",
       "\\item -2124313716\n",
       "\\item 448715549\n",
       "\\item -149754054\n",
       "\\item -1081506957\n",
       "\\item -532801000\n",
       "\\item -752476871\n",
       "\\item 574245798\n",
       "\\item -1439956497\n",
       "\\item -2092585116\n",
       "\\item 1478067989\n",
       "\\item 1600273618\n",
       "\\item -1684840917\n",
       "\\item 1861588848\n",
       "\\item -355797839\n",
       "\\item 1201925822\n",
       "\\item -1608264665\n",
       "\\item -492970436\n",
       "\\item 1525678605\n",
       "\\item 72961386\n",
       "\\item 1373331427\n",
       "\\item 275599304\n",
       "\\item 73272105\n",
       "\\item 1359545558\n",
       "\\item 1582153055\n",
       "\\item 876443668\n",
       "\\item 1818635269\n",
       "\\item 705855746\n",
       "\\item 576743579\n",
       "\\item -719412448\n",
       "\\item -621723487\n",
       "\\item -736498194\n",
       "\\item 398892439\n",
       "\\item -1113334548\n",
       "\\item 190571773\n",
       "\\item -1472972902\n",
       "\\item -1875027885\n",
       "\\item -152112776\n",
       "\\item -787319527\n",
       "\\item -791475706\n",
       "\\item -91794225\n",
       "\\item -763597628\n",
       "\\item 1273826549\n",
       "\\item -479145678\n",
       "\\item -1389818101\n",
       "\\item -827352368\n",
       "\\item 139167889\n",
       "\\item 50116894\n",
       "\\item -219888889\n",
       "\\item -501315684\n",
       "\\item 653362157\n",
       "\\item -150557238\n",
       "\\item -722047805\n",
       "\\item 2040397608\n",
       "\\item 1755470601\n",
       "\\item 1672174390\n",
       "\\item -552613825\n",
       "\\item 890080628\n",
       "\\item -1182979611\n",
       "\\item 239086946\n",
       "\\item -652979845\n",
       "\\item 786660992\n",
       "\\item -1743205247\n",
       "\\item -624996274\n",
       "\\item 848643191\n",
       "\\item 1517870668\n",
       "\\item 1952440029\n",
       "\\item -302796806\n",
       "\\item -1676829389\n",
       "\\item 1159035096\n",
       "\\item -500384519\n",
       "\\item 543488102\n",
       "\\item 165549999\n",
       "\\item 1169938980\n",
       "\\item 1000702677\n",
       "\\item -1375496814\n",
       "\\item 487141355\n",
       "\\item -407548368\n",
       "\\item 257428593\n",
       "\\item -829115522\n",
       "\\item -1381031449\n",
       "\\item 467525884\n",
       "\\item 1981150669\n",
       "\\item -1562493398\n",
       "\\item -213259869\n",
       "\\item 2091933320\n",
       "\\item 747674345\n",
       "\\item -1567432298\n",
       "\\item -2035727585\n",
       "\\item -1324199212\n",
       "\\item 213194693\n",
       "\\item 1996360130\n",
       "\\item 967748187\n",
       "\\item -1176499744\n",
       "\\item 1120421985\n",
       "\\item -294657362\n",
       "\\item -2134287529\n",
       "\\item -1237807188\n",
       "\\item 1514326205\n",
       "\\item 1953060954\n",
       "\\item -365800941\n",
       "\\item 1787408440\n",
       "\\item -26413863\n",
       "\\item 981997254\n",
       "\\item -555201905\n",
       "\\item -1772357756\n",
       "\\item 180021429\n",
       "\\item -30242318\n",
       "\\item -1452556085\n",
       "\\item -455167600\n",
       "\\item 1189642321\n",
       "\\item 386129374\n",
       "\\item 2117791943\n",
       "\\item 70511196\n",
       "\\item -355117139\n",
       "\\item 972553866\n",
       "\\item 34461315\n",
       "\\item 796683752\n",
       "\\item -970929463\n",
       "\\item 492279798\n",
       "\\item -1982714369\n",
       "\\item 933480500\n",
       "\\item -1484393051\n",
       "\\item -379316702\n",
       "\\item 275215163\n",
       "\\item -689158848\n",
       "\\item 1610069057\n",
       "\\item 566469902\n",
       "\\item -1642405321\n",
       "\\item -616894196\n",
       "\\item 2105320093\n",
       "\\item 1940730042\n",
       "\\item -1646037261\n",
       "\\item 1931712408\n",
       "\\item -1714742087\n",
       "\\item -1798019802\n",
       "\\item 1549659503\n",
       "\\item -1447770908\n",
       "\\item -660259179\n",
       "\\item 521514578\n",
       "\\item -1400323669\n",
       "\\item -586955536\n",
       "\\item -235609039\n",
       "\\item 350369854\n",
       "\\item 1849700263\n",
       "\\item -1149721668\n",
       "\\item -475551347\n",
       "\\item 2053893866\n",
       "\\item 1920611171\n",
       "\\item 333065544\n",
       "\\item 659221161\n",
       "\\item 798064214\n",
       "\\item -113080097\n",
       "\\item -2078675564\n",
       "\\item 124240773\n",
       "\\item -158707070\n",
       "\\item -982078437\n",
       "\\item -757059424\n",
       "\\item 1889472545\n",
       "\\item 1397921646\n",
       "\\item -2109613801\n",
       "\\item 2144862828\n",
       "\\item 1518693501\n",
       "\\item -1344849638\n",
       "\\item -161936429\n",
       "\\item -692375816\n",
       "\\item -1539361639\n",
       "\\item -253631610\n",
       "\\item 1089929295\n",
       "\\item -1860287932\n",
       "\\item 14451829\n",
       "\\item 1740964530\n",
       "\\item 479736459\n",
       "\\item -755201968\n",
       "\\item 1333082129\n",
       "\\item -859323746\n",
       "\\item -478749049\n",
       "\\item 235146524\n",
       "\\item 2063919981\n",
       "\\item -1370353846\n",
       "\\item -775487421\n",
       "\\item 396467368\n",
       "\\item -1106838903\n",
       "\\item -2133289802\n",
       "\\item -1045277761\n",
       "\\item 2110571252\n",
       "\\item -439189147\n",
       "\\item 998817506\n",
       "\\item 1561613563\n",
       "\\item -426521600\n",
       "\\item -239707135\n",
       "\\item 766818766\n",
       "\\item -2131345417\n",
       "\\item 107463628\n",
       "\\item 701834845\n",
       "\\item 2030006650\n",
       "\\item 1321930931\n",
       "\\item 2032694872\n",
       "\\item -1983824775\n",
       "\\item 1548259814\n",
       "\\item 661357359\n",
       "\\item -1980731484\n",
       "\\item 450411093\n",
       "\\item 995657490\n",
       "\\item -1949166741\n",
       "\\item -1247741008\n",
       "\\item -1704887311\n",
       "\\item 256670974\n",
       "\\item -1617494681\n",
       "\\item 1949181564\n",
       "\\item -2023416499\n",
       "\\item -1413324886\n",
       "\\item -919847645\n",
       "\\item -1800750072\n",
       "\\item 1951201897\n",
       "\\item 80010006\n",
       "\\item -1411805537\n",
       "\\item 940853332\n",
       "\\item 943599429\n",
       "\\item 1635212098\n",
       "\\item 2004381147\n",
       "\\item 920590176\n",
       "\\item 1547016161\n",
       "\\item 662834222\n",
       "\\item 1240471255\n",
       "\\item 2101490988\n",
       "\\item -538718147\n",
       "\\item -1422009894\n",
       "\\item 510756243\n",
       "\\item -1438421576\n",
       "\\item 843658329\n",
       "\\item 815820870\n",
       "\\item -2039253489\n",
       "\\item -41726716\n",
       "\\item -99491787\n",
       "\\item 149437298\n",
       "\\item 678323275\n",
       "\\item 1607016208\n",
       "\\item 162639825\n",
       "\\item 2030593886\n",
       "\\item -1067938745\n",
       "\\item 307163100\n",
       "\\item -1690288339\n",
       "\\item -724246518\n",
       "\\item 501344771\n",
       "\\item 1355647848\n",
       "\\item -1340806583\n",
       "\\item -85044874\n",
       "\\item 1550858623\n",
       "\\item -230130252\n",
       "\\item 807587109\n",
       "\\item 493758370\n",
       "\\item 1356527291\n",
       "\\item -728100160\n",
       "\\item 622117825\n",
       "\\item -2091741554\n",
       "\\item -387490377\n",
       "\\item -1631627636\n",
       "\\item 757688861\n",
       "\\item -1364561350\n",
       "\\item -125539725\n",
       "\\item 635704600\n",
       "\\item 30350393\n",
       "\\item 327253670\n",
       "\\item -1329145617\n",
       "\\item -2127635868\n",
       "\\item -1375734251\n",
       "\\item 1267474386\n",
       "\\item -1130027733\n",
       "\\item -1739787664\n",
       "\\item -799157327\n",
       "\\item 1922269630\n",
       "\\item -1082946777\n",
       "\\item -1195480772\n",
       "\\item 84824333\n",
       "\\item 396464234\n",
       "\\item -1323301149\n",
       "\\item 2111965896\n",
       "\\item 1398196777\n",
       "\\item -286459946\n",
       "\\item 1412322399\n",
       "\\item 398549780\n",
       "\\item 989354757\n",
       "\\item 814031874\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 10403\n",
       "2. 624\n",
       "3. -1564105589\n",
       "4. 103469648\n",
       "5. -280462831\n",
       "6. -984769378\n",
       "7. -1934069625\n",
       "8. 2112878364\n",
       "9. -3060371\n",
       "10. -923367094\n",
       "11. -72437181\n",
       "12. 473245352\n",
       "13. 1882094729\n",
       "14. -1374310730\n",
       "15. 804398527\n",
       "16. -695079692\n",
       "17. 685187941\n",
       "18. -998737694\n",
       "19. -344045829\n",
       "20. 1152685568\n",
       "21. -969269759\n",
       "22. -837741618\n",
       "23. -176401929\n",
       "24. 917384652\n",
       "25. -811988899\n",
       "26. 421686138\n",
       "27. 1266631347\n",
       "28. 771653720\n",
       "29. 1101610617\n",
       "30. 1798056934\n",
       "31. 1015010607\n",
       "32. -983557724\n",
       "33. 149281877\n",
       "34. -1466515182\n",
       "35. 1771603307\n",
       "36. -749451856\n",
       "37. -944390671\n",
       "38. -450930946\n",
       "39. 1753321319\n",
       "40. -747696004\n",
       "41. 71466829\n",
       "42. 1224989098\n",
       "43. -2083721437\n",
       "44. -496810488\n",
       "45. -1709867927\n",
       "46. -152111850\n",
       "47. -723361633\n",
       "48. 1489924692\n",
       "49. 192139589\n",
       "50. -559671998\n",
       "51. -1279565861\n",
       "52. -792403616\n",
       "53. 242899425\n",
       "54. 678127150\n",
       "55. 945760471\n",
       "56. 572366636\n",
       "57. 1912189501\n",
       "58. -1522674726\n",
       "59. 1243527059\n",
       "60. -1585547336\n",
       "61. 907163225\n",
       "62. 1873873478\n",
       "63. 2022754319\n",
       "64. -1373112572\n",
       "65. 1955594805\n",
       "66. -1448905358\n",
       "67. -1706174901\n",
       "68. 1518430480\n",
       "69. 1963389393\n",
       "70. 44581214\n",
       "71. -311681465\n",
       "72. -1151018532\n",
       "73. 145662253\n",
       "74. 1932745226\n",
       "75. 901487619\n",
       "76. 707466600\n",
       "77. 167668809\n",
       "78. 1485138806\n",
       "79. 348261247\n",
       "80. -2055755852\n",
       "81. -1677103323\n",
       "82. -581443166\n",
       "83. -1753814853\n",
       "84. 1019534528\n",
       "85. -2053470783\n",
       "86. 1531504782\n",
       "87. -1245745225\n",
       "88. -1297104756\n",
       "89. -1005564899\n",
       "90. 554134586\n",
       "91. 1068145779\n",
       "92. 1107566360\n",
       "93. 838409785\n",
       "94. -918611802\n",
       "95. 1953311471\n",
       "96. -242711452\n",
       "97. -579921899\n",
       "98. 239360466\n",
       "99. 1058903851\n",
       "100. -1567998864\n",
       "101. 1781798321\n",
       "102. -964666434\n",
       "103. -718267097\n",
       "104. 1177113404\n",
       "105. -1685212403\n",
       "106. 1973226090\n",
       "107. 850573539\n",
       "108. 1701090504\n",
       "109. -505328599\n",
       "110. -1636757034\n",
       "111. -1337383329\n",
       "112. 132484372\n",
       "113. -2012218107\n",
       "114. -1045701118\n",
       "115. -1360469605\n",
       "116. -980645856\n",
       "117. -594370143\n",
       "118. -1253991698\n",
       "119. 357901975\n",
       "120. -1900244500\n",
       "121. 1918227965\n",
       "122. -1063832422\n",
       "123. 458944851\n",
       "124. 2003269240\n",
       "125. 1431696921\n",
       "126. -1452386554\n",
       "127. -1630732849\n",
       "128. -1864777276\n",
       "129. -822403595\n",
       "130. -1651413454\n",
       "131. -29374453\n",
       "132. -1639530544\n",
       "133. 372582801\n",
       "134. -1522555362\n",
       "135. 897944583\n",
       "136. 806648988\n",
       "137. 123188461\n",
       "138. 173599434\n",
       "139. -1209383485\n",
       "140. 1911014440\n",
       "141. -1078584311\n",
       "142. -532027338\n",
       "143. 1143976255\n",
       "144. -1217387916\n",
       "145. -1191216411\n",
       "146. -1732769182\n",
       "147. -1370928517\n",
       "148. -1812733056\n",
       "149. -1067799167\n",
       "150. 1357741390\n",
       "151. 1524125047\n",
       "152. 144446284\n",
       "153. -448639011\n",
       "154. 1141189882\n",
       "155. -395856333\n",
       "156. 360742360\n",
       "157. 1008778745\n",
       "158. -1915304602\n",
       "159. 1114128559\n",
       "160. -1183600860\n",
       "161. 279712725\n",
       "162. 715305618\n",
       "163. 434923755\n",
       "164. 747565872\n",
       "165. -469619343\n",
       "166. -545382274\n",
       "167. -2145096985\n",
       "168. -511814148\n",
       "169. 1384425165\n",
       "170. 2004810538\n",
       "171. 513426083\n",
       "172. -1718836344\n",
       "173. -1116414999\n",
       "174. -2119700842\n",
       "175. 1227729951\n",
       "176. -1754306604\n",
       "177. 1414523077\n",
       "178. 2073323202\n",
       "179. -439344293\n",
       "180. -1127026976\n",
       "181. -638932639\n",
       "182. 350523310\n",
       "183. -436149161\n",
       "184. 514213036\n",
       "185. 1095612861\n",
       "186. -144091814\n",
       "187. -838276333\n",
       "188. 1546073400\n",
       "189. -28215847\n",
       "190. 1074815942\n",
       "191. -2047413361\n",
       "192. -995210108\n",
       "193. -1510344267\n",
       "194. -1802492174\n",
       "195. 1885043147\n",
       "196. 406509200\n",
       "197. 982720849\n",
       "198. -2116826402\n",
       "199. -2101036601\n",
       "200. 1858002780\n",
       "201. 1066174637\n",
       "202. -1893254262\n",
       "203. -604328061\n",
       "204. -1842662680\n",
       "205. 1897237449\n",
       "206. 841164022\n",
       "207. 335222527\n",
       "208. -683975372\n",
       "209. -1149679963\n",
       "210. -1889995998\n",
       "211. 1102408763\n",
       "212. 1090628160\n",
       "213. -835021503\n",
       "214. -1279340018\n",
       "215. 1921444663\n",
       "216. 2066949644\n",
       "217. 1727009693\n",
       "218. -1294225990\n",
       "219. 259428339\n",
       "220. -147612520\n",
       "221. 803216825\n",
       "222. -709676506\n",
       "223. 1815156335\n",
       "224. 937531876\n",
       "225. -832778347\n",
       "226. -965620910\n",
       "227. 2076506795\n",
       "228. 404908528\n",
       "229. 2095056177\n",
       "230. 1691919678\n",
       "231. 1730050215\n",
       "232. -1741809476\n",
       "233. 1290230413\n",
       "234. -1352029206\n",
       "235. -2126279581\n",
       "236. 2107339336\n",
       "237. -326095959\n",
       "238. -313291946\n",
       "239. -716181025\n",
       "240. -768867692\n",
       "241. -1946971003\n",
       "242. 85831554\n",
       "243. 1244734747\n",
       "244. 223876512\n",
       "245. 1044541729\n",
       "246. -1407957906\n",
       "247. 404906519\n",
       "248. 1956296556\n",
       "249. -224305795\n",
       "250. -629918182\n",
       "251. 199795923\n",
       "252. -25316360\n",
       "253. -523979367\n",
       "254. -1336463226\n",
       "255. -741430961\n",
       "256. -999975100\n",
       "257. 88905077\n",
       "258. -1218469966\n",
       "259. 1382083467\n",
       "260. -820138672\n",
       "261. 165730577\n",
       "262. 757378974\n",
       "263. -1293310073\n",
       "264. -903609828\n",
       "265. -1257431955\n",
       "266. -1034007478\n",
       "267. -1146300093\n",
       "268. -373988952\n",
       "269. -1109607543\n",
       "270. -86957642\n",
       "271. -1713095489\n",
       "272. 261707764\n",
       "273. -1623797147\n",
       "274. 435854306\n",
       "275. 595283451\n",
       "276. -89247488\n",
       "277. -956678911\n",
       "278. 1216145102\n",
       "279. 1250642167\n",
       "280. 221575372\n",
       "281. 1020893021\n",
       "282. 1581969018\n",
       "283. 1050094003\n",
       "284. -170034344\n",
       "285. -1661518471\n",
       "286. 2106875622\n",
       "287. 2005380143\n",
       "288. 1200768164\n",
       "289. 37833557\n",
       "290. 1785832466\n",
       "291. -1503179669\n",
       "292. -872111952\n",
       "293. 1015913713\n",
       "294. 1263528446\n",
       "295. 1205749351\n",
       "296. 486054780\n",
       "297. 1853214285\n",
       "298. 1042095274\n",
       "299. 1416533539\n",
       "300. -799997688\n",
       "301. -286049431\n",
       "302. -298588138\n",
       "303. 1248851871\n",
       "304. 1121672532\n",
       "305. 180027461\n",
       "306. 386381890\n",
       "307. -1916016933\n",
       "308. -841221024\n",
       "309. 22673633\n",
       "310. -1617905362\n",
       "311. -646340649\n",
       "312. -212211156\n",
       "313. 1511047485\n",
       "314. -1166551334\n",
       "315. 1052384915\n",
       "316. -852823368\n",
       "317. 1819260249\n",
       "318. 922926406\n",
       "319. -300671217\n",
       "320. -893410812\n",
       "321. -1196232395\n",
       "322. -289417102\n",
       "323. -972022453\n",
       "324. -1985002480\n",
       "325. 1809731793\n",
       "326. -68004770\n",
       "327. 1672762695\n",
       "328. 1426318556\n",
       "329. 731476013\n",
       "330. 616439050\n",
       "331. 817939203\n",
       "332. -1756999576\n",
       "333. 97233737\n",
       "334. -1491870090\n",
       "335. -1414847873\n",
       "336. 1263145652\n",
       "337. 536354341\n",
       "338. 1365050530\n",
       "339. -447025221\n",
       "340. 934901696\n",
       "341. -2108054335\n",
       "342. -1813529714\n",
       "343. -257595721\n",
       "344. -2124313716\n",
       "345. 448715549\n",
       "346. -149754054\n",
       "347. -1081506957\n",
       "348. -532801000\n",
       "349. -752476871\n",
       "350. 574245798\n",
       "351. -1439956497\n",
       "352. -2092585116\n",
       "353. 1478067989\n",
       "354. 1600273618\n",
       "355. -1684840917\n",
       "356. 1861588848\n",
       "357. -355797839\n",
       "358. 1201925822\n",
       "359. -1608264665\n",
       "360. -492970436\n",
       "361. 1525678605\n",
       "362. 72961386\n",
       "363. 1373331427\n",
       "364. 275599304\n",
       "365. 73272105\n",
       "366. 1359545558\n",
       "367. 1582153055\n",
       "368. 876443668\n",
       "369. 1818635269\n",
       "370. 705855746\n",
       "371. 576743579\n",
       "372. -719412448\n",
       "373. -621723487\n",
       "374. -736498194\n",
       "375. 398892439\n",
       "376. -1113334548\n",
       "377. 190571773\n",
       "378. -1472972902\n",
       "379. -1875027885\n",
       "380. -152112776\n",
       "381. -787319527\n",
       "382. -791475706\n",
       "383. -91794225\n",
       "384. -763597628\n",
       "385. 1273826549\n",
       "386. -479145678\n",
       "387. -1389818101\n",
       "388. -827352368\n",
       "389. 139167889\n",
       "390. 50116894\n",
       "391. -219888889\n",
       "392. -501315684\n",
       "393. 653362157\n",
       "394. -150557238\n",
       "395. -722047805\n",
       "396. 2040397608\n",
       "397. 1755470601\n",
       "398. 1672174390\n",
       "399. -552613825\n",
       "400. 890080628\n",
       "401. -1182979611\n",
       "402. 239086946\n",
       "403. -652979845\n",
       "404. 786660992\n",
       "405. -1743205247\n",
       "406. -624996274\n",
       "407. 848643191\n",
       "408. 1517870668\n",
       "409. 1952440029\n",
       "410. -302796806\n",
       "411. -1676829389\n",
       "412. 1159035096\n",
       "413. -500384519\n",
       "414. 543488102\n",
       "415. 165549999\n",
       "416. 1169938980\n",
       "417. 1000702677\n",
       "418. -1375496814\n",
       "419. 487141355\n",
       "420. -407548368\n",
       "421. 257428593\n",
       "422. -829115522\n",
       "423. -1381031449\n",
       "424. 467525884\n",
       "425. 1981150669\n",
       "426. -1562493398\n",
       "427. -213259869\n",
       "428. 2091933320\n",
       "429. 747674345\n",
       "430. -1567432298\n",
       "431. -2035727585\n",
       "432. -1324199212\n",
       "433. 213194693\n",
       "434. 1996360130\n",
       "435. 967748187\n",
       "436. -1176499744\n",
       "437. 1120421985\n",
       "438. -294657362\n",
       "439. -2134287529\n",
       "440. -1237807188\n",
       "441. 1514326205\n",
       "442. 1953060954\n",
       "443. -365800941\n",
       "444. 1787408440\n",
       "445. -26413863\n",
       "446. 981997254\n",
       "447. -555201905\n",
       "448. -1772357756\n",
       "449. 180021429\n",
       "450. -30242318\n",
       "451. -1452556085\n",
       "452. -455167600\n",
       "453. 1189642321\n",
       "454. 386129374\n",
       "455. 2117791943\n",
       "456. 70511196\n",
       "457. -355117139\n",
       "458. 972553866\n",
       "459. 34461315\n",
       "460. 796683752\n",
       "461. -970929463\n",
       "462. 492279798\n",
       "463. -1982714369\n",
       "464. 933480500\n",
       "465. -1484393051\n",
       "466. -379316702\n",
       "467. 275215163\n",
       "468. -689158848\n",
       "469. 1610069057\n",
       "470. 566469902\n",
       "471. -1642405321\n",
       "472. -616894196\n",
       "473. 2105320093\n",
       "474. 1940730042\n",
       "475. -1646037261\n",
       "476. 1931712408\n",
       "477. -1714742087\n",
       "478. -1798019802\n",
       "479. 1549659503\n",
       "480. -1447770908\n",
       "481. -660259179\n",
       "482. 521514578\n",
       "483. -1400323669\n",
       "484. -586955536\n",
       "485. -235609039\n",
       "486. 350369854\n",
       "487. 1849700263\n",
       "488. -1149721668\n",
       "489. -475551347\n",
       "490. 2053893866\n",
       "491. 1920611171\n",
       "492. 333065544\n",
       "493. 659221161\n",
       "494. 798064214\n",
       "495. -113080097\n",
       "496. -2078675564\n",
       "497. 124240773\n",
       "498. -158707070\n",
       "499. -982078437\n",
       "500. -757059424\n",
       "501. 1889472545\n",
       "502. 1397921646\n",
       "503. -2109613801\n",
       "504. 2144862828\n",
       "505. 1518693501\n",
       "506. -1344849638\n",
       "507. -161936429\n",
       "508. -692375816\n",
       "509. -1539361639\n",
       "510. -253631610\n",
       "511. 1089929295\n",
       "512. -1860287932\n",
       "513. 14451829\n",
       "514. 1740964530\n",
       "515. 479736459\n",
       "516. -755201968\n",
       "517. 1333082129\n",
       "518. -859323746\n",
       "519. -478749049\n",
       "520. 235146524\n",
       "521. 2063919981\n",
       "522. -1370353846\n",
       "523. -775487421\n",
       "524. 396467368\n",
       "525. -1106838903\n",
       "526. -2133289802\n",
       "527. -1045277761\n",
       "528. 2110571252\n",
       "529. -439189147\n",
       "530. 998817506\n",
       "531. 1561613563\n",
       "532. -426521600\n",
       "533. -239707135\n",
       "534. 766818766\n",
       "535. -2131345417\n",
       "536. 107463628\n",
       "537. 701834845\n",
       "538. 2030006650\n",
       "539. 1321930931\n",
       "540. 2032694872\n",
       "541. -1983824775\n",
       "542. 1548259814\n",
       "543. 661357359\n",
       "544. -1980731484\n",
       "545. 450411093\n",
       "546. 995657490\n",
       "547. -1949166741\n",
       "548. -1247741008\n",
       "549. -1704887311\n",
       "550. 256670974\n",
       "551. -1617494681\n",
       "552. 1949181564\n",
       "553. -2023416499\n",
       "554. -1413324886\n",
       "555. -919847645\n",
       "556. -1800750072\n",
       "557. 1951201897\n",
       "558. 80010006\n",
       "559. -1411805537\n",
       "560. 940853332\n",
       "561. 943599429\n",
       "562. 1635212098\n",
       "563. 2004381147\n",
       "564. 920590176\n",
       "565. 1547016161\n",
       "566. 662834222\n",
       "567. 1240471255\n",
       "568. 2101490988\n",
       "569. -538718147\n",
       "570. -1422009894\n",
       "571. 510756243\n",
       "572. -1438421576\n",
       "573. 843658329\n",
       "574. 815820870\n",
       "575. -2039253489\n",
       "576. -41726716\n",
       "577. -99491787\n",
       "578. 149437298\n",
       "579. 678323275\n",
       "580. 1607016208\n",
       "581. 162639825\n",
       "582. 2030593886\n",
       "583. -1067938745\n",
       "584. 307163100\n",
       "585. -1690288339\n",
       "586. -724246518\n",
       "587. 501344771\n",
       "588. 1355647848\n",
       "589. -1340806583\n",
       "590. -85044874\n",
       "591. 1550858623\n",
       "592. -230130252\n",
       "593. 807587109\n",
       "594. 493758370\n",
       "595. 1356527291\n",
       "596. -728100160\n",
       "597. 622117825\n",
       "598. -2091741554\n",
       "599. -387490377\n",
       "600. -1631627636\n",
       "601. 757688861\n",
       "602. -1364561350\n",
       "603. -125539725\n",
       "604. 635704600\n",
       "605. 30350393\n",
       "606. 327253670\n",
       "607. -1329145617\n",
       "608. -2127635868\n",
       "609. -1375734251\n",
       "610. 1267474386\n",
       "611. -1130027733\n",
       "612. -1739787664\n",
       "613. -799157327\n",
       "614. 1922269630\n",
       "615. -1082946777\n",
       "616. -1195480772\n",
       "617. 84824333\n",
       "618. 396464234\n",
       "619. -1323301149\n",
       "620. 2111965896\n",
       "621. 1398196777\n",
       "622. -286459946\n",
       "623. 1412322399\n",
       "624. 398549780\n",
       "625. 989354757\n",
       "626. 814031874\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1]       10403         624 -1564105589   103469648  -280462831  -984769378\n",
       "  [7] -1934069625  2112878364    -3060371  -923367094   -72437181   473245352\n",
       " [13]  1882094729 -1374310730   804398527  -695079692   685187941  -998737694\n",
       " [19]  -344045829  1152685568  -969269759  -837741618  -176401929   917384652\n",
       " [25]  -811988899   421686138  1266631347   771653720  1101610617  1798056934\n",
       " [31]  1015010607  -983557724   149281877 -1466515182  1771603307  -749451856\n",
       " [37]  -944390671  -450930946  1753321319  -747696004    71466829  1224989098\n",
       " [43] -2083721437  -496810488 -1709867927  -152111850  -723361633  1489924692\n",
       " [49]   192139589  -559671998 -1279565861  -792403616   242899425   678127150\n",
       " [55]   945760471   572366636  1912189501 -1522674726  1243527059 -1585547336\n",
       " [61]   907163225  1873873478  2022754319 -1373112572  1955594805 -1448905358\n",
       " [67] -1706174901  1518430480  1963389393    44581214  -311681465 -1151018532\n",
       " [73]   145662253  1932745226   901487619   707466600   167668809  1485138806\n",
       " [79]   348261247 -2055755852 -1677103323  -581443166 -1753814853  1019534528\n",
       " [85] -2053470783  1531504782 -1245745225 -1297104756 -1005564899   554134586\n",
       " [91]  1068145779  1107566360   838409785  -918611802  1953311471  -242711452\n",
       " [97]  -579921899   239360466  1058903851 -1567998864  1781798321  -964666434\n",
       "[103]  -718267097  1177113404 -1685212403  1973226090   850573539  1701090504\n",
       "[109]  -505328599 -1636757034 -1337383329   132484372 -2012218107 -1045701118\n",
       "[115] -1360469605  -980645856  -594370143 -1253991698   357901975 -1900244500\n",
       "[121]  1918227965 -1063832422   458944851  2003269240  1431696921 -1452386554\n",
       "[127] -1630732849 -1864777276  -822403595 -1651413454   -29374453 -1639530544\n",
       "[133]   372582801 -1522555362   897944583   806648988   123188461   173599434\n",
       "[139] -1209383485  1911014440 -1078584311  -532027338  1143976255 -1217387916\n",
       "[145] -1191216411 -1732769182 -1370928517 -1812733056 -1067799167  1357741390\n",
       "[151]  1524125047   144446284  -448639011  1141189882  -395856333   360742360\n",
       "[157]  1008778745 -1915304602  1114128559 -1183600860   279712725   715305618\n",
       "[163]   434923755   747565872  -469619343  -545382274 -2145096985  -511814148\n",
       "[169]  1384425165  2004810538   513426083 -1718836344 -1116414999 -2119700842\n",
       "[175]  1227729951 -1754306604  1414523077  2073323202  -439344293 -1127026976\n",
       "[181]  -638932639   350523310  -436149161   514213036  1095612861  -144091814\n",
       "[187]  -838276333  1546073400   -28215847  1074815942 -2047413361  -995210108\n",
       "[193] -1510344267 -1802492174  1885043147   406509200   982720849 -2116826402\n",
       "[199] -2101036601  1858002780  1066174637 -1893254262  -604328061 -1842662680\n",
       "[205]  1897237449   841164022   335222527  -683975372 -1149679963 -1889995998\n",
       "[211]  1102408763  1090628160  -835021503 -1279340018  1921444663  2066949644\n",
       "[217]  1727009693 -1294225990   259428339  -147612520   803216825  -709676506\n",
       "[223]  1815156335   937531876  -832778347  -965620910  2076506795   404908528\n",
       "[229]  2095056177  1691919678  1730050215 -1741809476  1290230413 -1352029206\n",
       "[235] -2126279581  2107339336  -326095959  -313291946  -716181025  -768867692\n",
       "[241] -1946971003    85831554  1244734747   223876512  1044541729 -1407957906\n",
       "[247]   404906519  1956296556  -224305795  -629918182   199795923   -25316360\n",
       "[253]  -523979367 -1336463226  -741430961  -999975100    88905077 -1218469966\n",
       "[259]  1382083467  -820138672   165730577   757378974 -1293310073  -903609828\n",
       "[265] -1257431955 -1034007478 -1146300093  -373988952 -1109607543   -86957642\n",
       "[271] -1713095489   261707764 -1623797147   435854306   595283451   -89247488\n",
       "[277]  -956678911  1216145102  1250642167   221575372  1020893021  1581969018\n",
       "[283]  1050094003  -170034344 -1661518471  2106875622  2005380143  1200768164\n",
       "[289]    37833557  1785832466 -1503179669  -872111952  1015913713  1263528446\n",
       "[295]  1205749351   486054780  1853214285  1042095274  1416533539  -799997688\n",
       "[301]  -286049431  -298588138  1248851871  1121672532   180027461   386381890\n",
       "[307] -1916016933  -841221024    22673633 -1617905362  -646340649  -212211156\n",
       "[313]  1511047485 -1166551334  1052384915  -852823368  1819260249   922926406\n",
       "[319]  -300671217  -893410812 -1196232395  -289417102  -972022453 -1985002480\n",
       "[325]  1809731793   -68004770  1672762695  1426318556   731476013   616439050\n",
       "[331]   817939203 -1756999576    97233737 -1491870090 -1414847873  1263145652\n",
       "[337]   536354341  1365050530  -447025221   934901696 -2108054335 -1813529714\n",
       "[343]  -257595721 -2124313716   448715549  -149754054 -1081506957  -532801000\n",
       "[349]  -752476871   574245798 -1439956497 -2092585116  1478067989  1600273618\n",
       "[355] -1684840917  1861588848  -355797839  1201925822 -1608264665  -492970436\n",
       "[361]  1525678605    72961386  1373331427   275599304    73272105  1359545558\n",
       "[367]  1582153055   876443668  1818635269   705855746   576743579  -719412448\n",
       "[373]  -621723487  -736498194   398892439 -1113334548   190571773 -1472972902\n",
       "[379] -1875027885  -152112776  -787319527  -791475706   -91794225  -763597628\n",
       "[385]  1273826549  -479145678 -1389818101  -827352368   139167889    50116894\n",
       "[391]  -219888889  -501315684   653362157  -150557238  -722047805  2040397608\n",
       "[397]  1755470601  1672174390  -552613825   890080628 -1182979611   239086946\n",
       "[403]  -652979845   786660992 -1743205247  -624996274   848643191  1517870668\n",
       "[409]  1952440029  -302796806 -1676829389  1159035096  -500384519   543488102\n",
       "[415]   165549999  1169938980  1000702677 -1375496814   487141355  -407548368\n",
       "[421]   257428593  -829115522 -1381031449   467525884  1981150669 -1562493398\n",
       "[427]  -213259869  2091933320   747674345 -1567432298 -2035727585 -1324199212\n",
       "[433]   213194693  1996360130   967748187 -1176499744  1120421985  -294657362\n",
       "[439] -2134287529 -1237807188  1514326205  1953060954  -365800941  1787408440\n",
       "[445]   -26413863   981997254  -555201905 -1772357756   180021429   -30242318\n",
       "[451] -1452556085  -455167600  1189642321   386129374  2117791943    70511196\n",
       "[457]  -355117139   972553866    34461315   796683752  -970929463   492279798\n",
       "[463] -1982714369   933480500 -1484393051  -379316702   275215163  -689158848\n",
       "[469]  1610069057   566469902 -1642405321  -616894196  2105320093  1940730042\n",
       "[475] -1646037261  1931712408 -1714742087 -1798019802  1549659503 -1447770908\n",
       "[481]  -660259179   521514578 -1400323669  -586955536  -235609039   350369854\n",
       "[487]  1849700263 -1149721668  -475551347  2053893866  1920611171   333065544\n",
       "[493]   659221161   798064214  -113080097 -2078675564   124240773  -158707070\n",
       "[499]  -982078437  -757059424  1889472545  1397921646 -2109613801  2144862828\n",
       "[505]  1518693501 -1344849638  -161936429  -692375816 -1539361639  -253631610\n",
       "[511]  1089929295 -1860287932    14451829  1740964530   479736459  -755201968\n",
       "[517]  1333082129  -859323746  -478749049   235146524  2063919981 -1370353846\n",
       "[523]  -775487421   396467368 -1106838903 -2133289802 -1045277761  2110571252\n",
       "[529]  -439189147   998817506  1561613563  -426521600  -239707135   766818766\n",
       "[535] -2131345417   107463628   701834845  2030006650  1321930931  2032694872\n",
       "[541] -1983824775  1548259814   661357359 -1980731484   450411093   995657490\n",
       "[547] -1949166741 -1247741008 -1704887311   256670974 -1617494681  1949181564\n",
       "[553] -2023416499 -1413324886  -919847645 -1800750072  1951201897    80010006\n",
       "[559] -1411805537   940853332   943599429  1635212098  2004381147   920590176\n",
       "[565]  1547016161   662834222  1240471255  2101490988  -538718147 -1422009894\n",
       "[571]   510756243 -1438421576   843658329   815820870 -2039253489   -41726716\n",
       "[577]   -99491787   149437298   678323275  1607016208   162639825  2030593886\n",
       "[583] -1067938745   307163100 -1690288339  -724246518   501344771  1355647848\n",
       "[589] -1340806583   -85044874  1550858623  -230130252   807587109   493758370\n",
       "[595]  1356527291  -728100160   622117825 -2091741554  -387490377 -1631627636\n",
       "[601]   757688861 -1364561350  -125539725   635704600    30350393   327253670\n",
       "[607] -1329145617 -2127635868 -1375734251  1267474386 -1130027733 -1739787664\n",
       "[613]  -799157327  1922269630 -1082946777 -1195480772    84824333   396464234\n",
       "[619] -1323301149  2111965896  1398196777  -286459946  1412322399   398549780\n",
       "[625]   989354757   814031874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ".Random.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e8026d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.585800305008888"
      ],
      "text/latex": [
       "0.585800305008888"
      ],
      "text/markdown": [
       "0.585800305008888"
      ],
      "text/plain": [
       "[1] 0.5858003"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6959c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
